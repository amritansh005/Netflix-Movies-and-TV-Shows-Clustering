# -*- coding: utf-8 -*-
"""Netflix Movies and TV Shows Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDIm0KAZ8-1_ZW2Pg1opzfoSN971Zs3e

# Data collection and **combination**
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install fuzzywuzzy # make sure fuzzywuzzy is installed
!pip install python-Levenshtein # install a levenshtein distance library for better performance
!pip install fuzzy
!pip install metaphone
!pip install sentence_transformers
!pip install rake-nltk
!pip install squarify
!pip install pycountry_convert


import pycountry_convert as pc
# %matplotlib inline
import squarify
from metaphone import doublemetaphone
import pandas as pd
import numpy as np
import string
from jellyfish import metaphone
from fuzzywuzzy import fuzz #import the entire fuzzywuzzy module
from fuzzywuzzy import process # you may need to import the process module for some functions
import ast
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.preprocessing import MinMaxScaler
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from rake_nltk import Rake
from sklearn.feature_extraction.text import TfidfVectorizer
from textblob import TextBlob
import plotly.express as px
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
import networkx as nx
from scipy.sparse import csr_matrix
import collections
import altair as alt
import scipy.sparse as sparse
from scipy.stats import chi2_contingency
from scipy.stats import pearsonr
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
from nltk.stem import WordNetLemmatizer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from scipy import sparse
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import MultiLabelBinarizer, OrdinalEncoder
from sklearn.metrics import silhouette_samples, silhouette_score
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from scipy.sparse import hstack
import matplotlib.cm as cm # import the module here
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from google.colab import drive
drive.mount('/content/drive')

df1=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Clustering projects/Netflix Movies and TV Shows Clustering/rotten_tomatoes_critic_reviews.csv')

df2=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Clustering projects/Netflix Movies and TV Shows Clustering/rotten_tomatoes_movies.csv')

df3=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Clustering projects/Netflix Movies and TV Shows Clustering/imdb_data.csv')

df4=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Clustering projects/Netflix Movies and TV Shows Clustering/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')

df1.head()

df1.info()

df1.dropna(inplace=True)

df1.head()

df1.info()

df1.drop_duplicates(inplace=True)

df1.drop_duplicates(subset=['rotten_tomatoes_link'], inplace=True)

df1.shape

df2.head()

df2.info()

df2.dropna(inplace=True)

df2.head()

df2.info()

df2.drop_duplicates(inplace=True)

df2.drop_duplicates(subset=['rotten_tomatoes_link'], inplace=True)

df2.shape

# Count the number of unique values in 'rotten_tomatoes_link' in each dataframe
print(df1['rotten_tomatoes_link'].nunique())

# Count the number of unique values in 'rotten_tomatoes_link' in each dataframe
print(df2['rotten_tomatoes_link'].nunique())

# Merge df1 and df2 on 'movie_title' using inner join
df5 = pd.merge(df1, df2, on='rotten_tomatoes_link', how='inner')

df5.head()

df5.info()

# Drop rows with null values in `df5` inplace
df5.dropna(inplace=True)

df5.rename(columns={'movie_title': 'title'}, inplace=True)

df5.head()

df5.info()

df5.drop_duplicates(inplace=True)

df5.shape

df3.head()

df3.info()

df3.drop(columns=['belongs_to_collection','homepage'], inplace=True)

df3.dropna(inplace=True)

df3.drop_duplicates(inplace=True)

df3.head()

df3.info()

df3.shape

df4.head()

df4.info()

df4.shape

# 1. Convert all rows of 'title' column to lowercase (if applicable)
df3['title'] = df3['title'].astype(str).str.lower()
df4['title'] = df4['title'].astype(str).str.lower()

# Merge DataFrames based on the exact lowercase 'title', keeping all rows from df4 (left DataFrame)
df6 = pd.merge(df4, df3, on='title', how='left')

# Display the merged DataFrame
print(df6)

df6.info()

df6.shape

#df6.drop(columns=['title_y'],inplace=True)

df6.rename(columns={'title_x': 'title'}, inplace=True)

# 1. Convert all rows of 'title' column to lowercase
df5['title'] = df5['title'].astype(str).str.lower()
df6['title'] = df6['title'].astype(str).str.lower()

# Merge DataFrames based on the exact lowercase 'title'
df7 = pd.merge(df6, df5, on='title', how='left')

# Display the merged DataFrame
print(df7)

df7.head()

df7.info()

df=df7

df.head()

df.info()

print(df['director'].loc[4])

print(type(df['director'].loc[4]))

print(df['crew'].loc[4])

print(type(df['crew'].loc[4]))

print(df['directors'].loc[4])

print(type(df['directors'].loc[4]))

# Assuming you have a DataFrame named 'df'
for directors_value in df['director']:
    print(directors_value)

# Define the function to extract director names, handling string conversion and potential errors
def extract_director_name(crew_list_str):
    try:
        crew_list = ast.literal_eval(crew_list_str)
        director_names = []
        for crew_member in crew_list:
            if crew_member['job'] == 'Director':
                director_names.append(crew_member['name'])
        return director_names
    except ValueError as e:
        print(f"Error parsing crew_list_str: {crew_list_str}")
        raise e

# Fill NaN values in 'crew' with an empty list representation
df['crew'] = df['crew'].fillna('[]')

# Fill NaN values in 'directors' with an empty string
df['directors'] = df['directors'].fillna('')

# Apply the function to the 'crew' column
df['names_of_director'] = df['crew'].apply(extract_director_name)

df.info()

# Fill NaN values in 'directors' with an empty string
df['directors'] = df['directors'].fillna('')

def fill_director_from_multiple_sources(row):
    if pd.isna(row['director']):  # Only update if 'director' is NaN
        if isinstance(row['names_of_director'], list) and row['names_of_director']:
            return ", ".join(row['names_of_director'])  # From 'crew' if available
        elif row['directors']:  # From 'directors' if available and 'crew' didn't provide any
            return row['directors']
        else:
            return row['director']  # Keep NaN if both sources are empty or invalid
    else:
        return row['director']  # Keep original value if 'director' is not NaN

# Apply the extended function row-wise and update the 'director' column
df['director'] = df.apply(fill_director_from_multiple_sources, axis=1)

df.info()

print(df['cast_x'].loc[3])

print(type(df['cast_x'].loc[3]))

print(df['cast_y'].loc[3])

print(type(df['cast_y'].loc[3]))

print(df['actors'].loc[3])

print(type(df['actors'].loc[3]))

# Assuming you have a DataFrame named 'df'
for cast_value in df['cast_x']:
    print(cast_value)

def extract_names(cast_y_str):
    try:
        cast_list = ast.literal_eval(cast_y_str)
        names = [member['name'] for member in cast_list]
        return ", ".join(names)
    except (ValueError, SyntaxError):  # Handle parsing errors, potentially due to NaN or malformed strings
        return ""

# Fill NaN values with empty strings
df['cast_y'] = df['cast_y'].fillna('[]')

# Apply the function to extract names and create the 'new_cast' column
df['new_cast'] = df['cast_y'].apply(extract_names)

# Fill NaN values in 'actors' with an empty string
df['actors'] = df['actors'].fillna('')

def replace_nan_with_multiple_sources(row):
    if pd.isna(row['cast_x']):
        if row['new_cast']:  # From 'new_cast' if available
            return row['new_cast']
        elif row['actors']:  # From 'actors' if available and 'new_cast' didn't provide any
            return row['actors']
        else:
            return row['cast_x']  # Keep NaN if both sources are empty
    else:
        return row['cast_x']  # Keep original value if 'cast_x' is not NaN

# Apply the extended function row-wise and update the 'cast_x' column
df['cast_x'] = df.apply(replace_nan_with_multiple_sources, axis=1)

# Assuming you have a DataFrame named 'df'
for cast_value in df['cast_x']:
    print(cast_value)

df.rename(columns={'cast_x': 'cast'}, inplace=True)

df.info()

print(df['production_countries'].loc[0])

print(type(df['production_countries'].loc[0]))

print(df['country'].loc[0])

print(type(df['country'].loc[0]))

# Assuming you have a DataFrame named 'df'
for cast_value in df['country']:
    print(cast_value)

def extract_country_names(production_countries_str):
    try:
        countries_list = ast.literal_eval(production_countries_str)
        names = [country['name'] for country in countries_list]
        return ", ".join(names)  # Join multiple country names with commas
    except (ValueError, SyntaxError):
        return ""

# Fill NaN values with empty strings in 'production_countries'
df['production_countries'] = df['production_countries'].fillna('[]')

# Apply the function to extract country names and create a temporary column
df['extracted_countries'] = df['production_countries'].apply(extract_country_names)

def replace_nan_with_extracted_countries(row):
    if pd.isna(row['country']) and row['extracted_countries'] != "":
        return row['extracted_countries']
    else:
        return row['country']

# Apply the function row-wise and update the 'country' column
df['country'] = df.apply(replace_nan_with_extracted_countries, axis=1)

# Assuming you have a DataFrame named 'df'
for cast_value in df['country']:
    print(cast_value)

df.info()

df.drop(columns=['crew','names_of_director','cast_y','extracted_countries','production_countries','new_cast','directors','actors'],inplace=True)

df.info()

"""<h2>So the values of date_added and streaming_release_date are different because there are movies which are old when netflix didnt existed but netflix has taken over those movies sometime later. So we can't combine these two columns(while inspection learnt it because dates were different so i thought is there a problem in code or some reason can be behind it). Infact same goes for rating because differnt platforms have different rating systems some have ratings out of 5 while others have rating out of 10. So we can't combine rating column as well.</h2>"""

# List of columns to keep
columns_to_keep = [
    'show_id', 'type', 'title', 'director', 'cast', 'country',
    'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description'
]

# Drop all columns not in the 'columns_to_keep' list
df = df[columns_to_keep]

df.head()

df.info()

new_df=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Clustering projects/Netflix Movies and TV Shows Clustering/netflix_titles.csv')

new_df.info()

print(df['show_id'].loc[0])

print(type(df['show_id'].loc[0]))

print(new_df['show_id'].loc[0])

print(type(new_df['show_id'].loc[0]))

# Merge df and new_df on 'show_id' using a left join
merged_df = pd.merge(df, new_df, on='show_id', how='left', suffixes=('', '_new'))

# Iterate over columns and update NaN values from 'df' with values from 'new_df'
for col in df.columns:
    if col != 'show_id':
        merged_df[col] = merged_df[col].fillna(merged_df[col + '_new'])

# Drop columns originating from 'new_df'
merged_df.drop(columns=[col for col in merged_df.columns if col.endswith('_new')], inplace=True)

df=merged_df

df.head()

df.info()

"""<h3>So the original dataset only had around some 7500-8000 values to deal with, which is not a very large number. So instead to dropping the rows i tried to find similar datasets so that i can gain the missing values out of those datasets and put them in my original dataset. So i downloaded imdb, rotten tomatoes and another netfix dataset from the internet and merged them with my original data set.</h3>
<h1>Results achieved:</h2>

<h3>From:

Column      -           Non-Null Count    


 0    show_id        -         7787 non-null   

 1    type            -        7787 non-null    

 2    title          -         7787 non-null    

 3    director        -        5398 non-null    

 4    cast            -        7069 non-null    

 5    country           -      7280 non-null    

 6    date_added       -       7777 non-null    

 7    release_year      -      7787 non-null    

 8    rating            -      7780 non-null   

 9    duration        -        7787 non-null    

 10   listed_in         -      7787 non-null    

 11   description          -   7787 non-null    </h3>
 <h3>To:

 Column        Non-Null Count  

 0   show_id  -     7840 non-null  

 1   type      -    7840 non-null  

 2   title     -    7840 non-null

 3   director   -   7106 non-null   

 4   cast        -  7781 non-null   

 5   country     -  7789 non-null  

 6   date_added  -  7840 non-null  

 7   release_year - 7840 non-null  
  
 8   rating  -      7840 non-null   

 9   duration  -    7840 non-null   

 10  listed_in   -  7840 non-null

 11  description   -7840 non-null  
  </h3>
"""

df.dropna(inplace=True)

#Drop duplicates
df.drop_duplicates(inplace=True)

"""# **Exploratory** data analysis"""

df.head()

df.info()

df['type'].nunique()

df['type'].unique()

# Count the occurrences of each type
type_counts = df['type'].value_counts()

# Create a bar chart
plt.figure(figsize=(8, 6))
plt.bar(type_counts.index, type_counts.values, color=['skyblue', 'salmon'])
plt.xlabel('Type')
plt.ylabel('Count')
plt.title('Distribution of Movies vs. TV Shows')
plt.show()

# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', colors=['skyblue', 'salmon'])
plt.title('Proportion of Movies vs. TV Shows')
plt.show()

# Convert 'date_added' to datetime format with the correct format
df['date_added'] = pd.to_datetime(df['date_added'].str.strip(), format='%B %d, %Y')

# Extract the year from 'date_added'
df['year_added'] = df['date_added'].dt.year

# Group by 'type' and 'year' and count the entries
content_counts = df.groupby(['type', 'year_added']).size().reset_index(name='count')

# Separate Movies and TV Shows
movies_counts = content_counts[content_counts['type'] == 'Movie']
tv_shows_counts = content_counts[content_counts['type'] == 'TV Show']

'''# Create a line chart
plt.figure(figsize=(10, 6))
plt.plot(movies_counts['year_added'], movies_counts['count'], label='Movies', marker='o')
plt.plot(tv_shows_counts['year_added'], tv_shows_counts['count'], label='TV Shows', marker='s')
plt.xlabel('Year added')
plt.ylabel('Number of Titles')
plt.title('Netflix Content Trend: Movies vs. TV Shows')
plt.legend()
plt.grid(True)
plt.show()'''

grouped_counts = df.groupby(['type', 'rating']).size().reset_index(name='count')

# Set seaborn style (optional but makes plots look nicer)
sns.set(style='whitegrid')

# Create the grouped bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x='rating', y='count', hue='type', data=grouped_counts)
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Ratings: Movies vs. TV Shows')
plt.show()

grouped_counts = df.groupby(['type', 'release_year']).size().reset_index(name='count')

# Create separate DataFrames for movies and TV shows
movies_data = grouped_counts[grouped_counts['type'] == 'Movie']
tv_shows_data = grouped_counts[grouped_counts['type'] == 'TV Show']

plt.figure(figsize=(10, 6))
plt.plot(movies_data['release_year'], movies_data['count'], label='Movies', marker='o')
plt.plot(tv_shows_data['release_year'], tv_shows_data['count'], label='TV Shows', marker='s')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.title('Distribution of Release Years: Movies vs. TV Shows')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=movies_data, x='release_year', bins=20, kde=True, label='Movies', color='skyblue')
sns.histplot(data=tv_shows_data, x='release_year', bins=20, kde=True, label='TV Shows', color='salmon')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.title('Distribution of Release Years: Movies vs. TV Shows')
plt.legend()
plt.show()

df['genres'] = df['listed_in'].str.split(', ')

grouped_genres = df.explode('genres').groupby(['type', 'genres']).size().reset_index(name='count')

# Create separate DataFrames for movies and TV shows
movies_data = grouped_genres[grouped_genres['type'] == 'Movie']
tv_shows_data = grouped_genres[grouped_genres['type'] == 'TV Show']

plt.figure(figsize=(12, 6))
sns.barplot(x='count', y='genres', hue='type', data=movies_data, palette='coolwarm')
plt.xlabel('Count')
plt.ylabel('Genres')
plt.title('Most Common Genres: Movies')
plt.show()

plt.figure(figsize=(12, 6))
sns.barplot(x='count', y='genres', hue='type', data=tv_shows_data, palette='coolwarm')
plt.xlabel('Count')
plt.ylabel('Genres')
plt.title('Most Common Genres: TV Shows')
plt.show()

# Calculate the length of each title
df['title_length'] = df['title'].str.len()

# Visualize the distribution for all titles
sns.histplot(data=df, x='title_length', kde=True)  # kde adds a density curve for smoother visualization
plt.title('Distribution of Title Lengths')
plt.xlabel('Title Length')
plt.ylabel('Count')
plt.show()

!pip install nltk
import nltk
import string
from collections import Counter
from nltk.corpus import stopwords

nltk.download('stopwords') # download stopwords resource

# 2. Title Analysis by Content Type

# Title Length Distribution (no changes needed here)
df['title_length'] = df['title'].str.len()
sns.histplot(data=df, x='title_length', hue='type', kde=True)
plt.title('Title Length Distribution by Type')
plt.show()

# Word Frequency Distribution (with stop word and punctuation removal, but preserving original titles)
def get_word_counts(text):
    stop_words = set(stopwords.words('english'))
    words = text.lower().translate(str.maketrans('', '', string.punctuation)).split()
    words = [word for word in words if word not in stop_words]
    return Counter(words)


# Create a new column 'processed_title' for the cleaned titles
df['processed_title'] = df['title'].astype(str).apply(get_word_counts)
df['num_words'] = df['processed_title'].apply(len)

sns.histplot(data=df, x='num_words', hue='type', kde=True)
plt.title('Word Frequency Distribution by Type')
plt.show()

# 3. Common Words or Phrases by Type (use the processed titles)
def get_top_words(word_counts_series, n=10):
    all_words = Counter()
    word_counts_series.apply(all_words.update)
    return all_words.most_common(n)

for content_type in df['type'].unique():
    type_df = df[df['type'] == content_type]
    top_words = get_top_words(type_df['processed_title'])  # Use processed_title here
    print(f"\nTop Words for {content_type}:")
    for word, count in top_words:
        print(f"{word}: {count}")

# Create separate DataFrames for each genre
genre_dfs = {genre: df[df['genres'].apply(lambda x: genre in x)] for genre in df['genres'].explode().unique()}

# Analyze keywords for each genre using TF-IDF
for genre, genre_df in genre_dfs.items():
    if not genre_df.empty:
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(genre_df['title'])

        # Get top keywords for the genre
        feature_names = vectorizer.get_feature_names_out()
        # Calculate the sum of tfidf scores for each word across all documents
        tfidf_sums = tfidf_matrix.sum(axis=0).A1
        top_keywords = [(feature_names[i], tfidf_sums[i]) for i in tfidf_sums.argsort()[::-1][:5]]  # Top 5 keywords

        print(f"\nTop Keywords for Genre '{genre}' obtained from title:")
        for keyword, score in top_keywords:
            print(f"{keyword}: {score:.4f}") # Removed item() as score is now a scalar

# Download stopwords if not already downloaded
nltk.download('stopwords')

# 1. Calculate title length
df['title_length'] = df['title'].str.len()

# 2. Group by release year and calculate mean title length
yearly_title_length = df.groupby('release_year')['title_length'].mean().reset_index()

# 3. Create line plot
chart = alt.Chart(yearly_title_length).mark_line().encode(
    x='release_year:T',
    y='title_length:Q',
    tooltip = ['release_year', 'title_length']
)

# 4. Add labels and title
chart = chart.properties(
    title='Average Title Length Over Time'
)

# 5. Display the plot
chart.save('average_title_length_over_time.json')

# 6. Group data by release year and aggregate titles
grouped_data = df.groupby('release_year')['title'].agg(list).reset_index()

# 7. Define function to get top words with stop word and punctuation removal
def get_word_counts(titles, n=10):
    stop_words = set(stopwords.words('english'))
    all_words = []
    for title in titles:
        words = title.lower().split()
        # Remove punctuation and stop words
        words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]
        all_words.extend([word for word in words if word and word not in stop_words])
    word_counts = Counter(all_words)
    return word_counts.most_common(n)

# 8. Apply function to get top words for each year
grouped_data['top_words'] = grouped_data['title'].apply(get_word_counts)

# 9. Print top words for each year
for index, row in grouped_data.iterrows():
    print(f"\nTop Words for {row['release_year']} obtained from title column:")
    for word, count in row['top_words']:
        print(f"{word}: {count}")

# Download NLTK resources if you haven't already
nltk.download('stopwords')
nltk.download('punkt')

# Tokenize titles and remove stop words
stop_words = set(stopwords.words('english'))

def tokenize_and_remove_stopwords(title):
    words = word_tokenize(title)
    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
    return filtered_words

df['words'] = df['title'].apply(tokenize_and_remove_stopwords)

# Count word frequencies for all titles
all_words = [word for words in df['words'] for word in words]
word_freq = Counter(all_words)

# Visualize top N most frequent words (adjust N as needed)
N = 20
most_common_words = word_freq.most_common(N)

plt.figure(figsize=(10, 6))
plt.bar(*zip(*most_common_words))
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Most Common Words in Titles (All)')
plt.xticks(rotation=45)
plt.show()

# Get word frequencies for Movies and TV Shows separately
movie_words = [word for words in df[df['type'] == 'Movie']['words'] for word in words]
tv_show_words = [word for words in df[df['type'] == 'TV Show']['words'] for word in words]
movie_word_freq = Counter(movie_words)
tv_show_word_freq = Counter(tv_show_words)

# Find common words and their frequencies
common_words = set(movie_word_freq) & set(tv_show_word_freq)
common_word_counts = {word: min(movie_word_freq[word], tv_show_word_freq[word]) for word in common_words}

# Get the top 20 most common words
most_common_shared_words = Counter(common_word_counts).most_common(20)

# Extract words and their frequencies for movies and TV shows separately
words = [word for word, _ in most_common_shared_words]
movie_freqs = [movie_word_freq[word] for word in words]
tv_show_freqs = [tv_show_word_freq[word] for word in words]

# Set up the bar chart with grouped bars
bar_width = 0.35
index = np.arange(len(words))

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar(index, movie_freqs, bar_width, label='Movie')
bars2 = ax.bar(index + bar_width, tv_show_freqs, bar_width, label='TV Show')

# Add labels, title, and legend
ax.set_xlabel('Word')
ax.set_ylabel('Frequency')
ax.set_title('Top 20 Most Common Words in title: Movies vs. TV Shows')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(words, rotation=45)
ax.legend()

plt.tight_layout()
plt.show()

# Calculate Sentiment Polarity
def get_sentiment(title):
    return TextBlob(title).sentiment.polarity

df['sentiment'] = df['title'].apply(get_sentiment)

# Group by Rating and Calculate Average Sentiment
average_sentiment_by_rating = df.groupby('rating')['sentiment'].mean()

# Visualize
average_sentiment_by_rating.plot(kind='bar')
plt.title('Average Sentiment by Rating of column title')
plt.xlabel('Rating')
plt.ylabel('Average Sentiment Polarity')
plt.show()

# Calculate Title Length
df['title_length'] = df['title'].str.len()

# Group by Rating and Calculate Average Title Length
average_title_length_by_rating = df.groupby('rating')['title_length'].mean()

# Visualize
average_title_length_by_rating.plot(kind='bar')
plt.title('Average Title Length by Rating')
plt.xlabel('Rating')
plt.ylabel('Average Title Length')
plt.show()

# Calculate title length
df['title_length'] = df['title'].str.len()

# Explode the DataFrame to have one row per genre per title
exploded_df = df.explode('genres')

# Group by genre and calculate average title length
average_title_length_by_genre = exploded_df.groupby('genres')['title_length'].mean().reset_index()

# Visualize
plt.figure(figsize=(12, 6))
sns.barplot(x='genres', y='title_length', data=average_title_length_by_genre)
plt.title('Average Title Length by Genre')
plt.xlabel('Genre')
plt.ylabel('Average Title Length')
plt.xticks(rotation=90)
plt.show()

# 3. Group by `year_added` and calculate mean `title_length`
yearly_title_length = df.groupby('year_added')['title_length'].mean()

# 4. Plot the trend
yearly_title_length.plot(figsize=(10, 6), title='Trend of Title Length Over Time')
plt.xlabel('Year Added')
plt.ylabel('Average Title Length')
plt.grid(True)
plt.show()

'''#The code aims to figure out if movies or TV shows that share the same director or actors tend to have similar-sounding titles.

# Load pre-trained Sentence Transformer model (if not already loaded)
model = SentenceTransformer('paraphrase-distilroberta-base-v2')

# Function to calculate similarity scores within a group
def calculate_similarity_scores(group):
    titles = group['title'].tolist()
    embeddings = model.encode(titles)
    similarity_matrix = cosine_similarity(embeddings)
    # Extract upper triangle to avoid duplicates and self-comparisons
    similarity_scores = similarity_matrix[np.triu_indices(len(similarity_matrix), k=1)]
    return similarity_scores

# 1. Calculate similarity scores for movies with the same director
director_similarity = df.groupby('director').apply(calculate_similarity_scores)

# 2. Calculate similarity scores for movies with the same cast members
# Explode the 'cast' column to handle multiple cast members per movie
df_exploded_cast = df.assign(cast=df['cast'].str.split(', ')).explode('cast')
cast_similarity = df_exploded_cast.groupby('cast').apply(calculate_similarity_scores)

# 3. Visualize the distributions
plt.figure(figsize=(12, 6))

# Plot for directors
sns.histplot(director_similarity.explode(), kde=True, label='Directors')

# Plot for cast members
sns.histplot(cast_similarity.explode(), kde=True, label='Cast Members', color='orange')

plt.title('Distribution of Title Similarity Scores for Directors and Cast Members')
plt.xlabel('Similarity Score')
plt.ylabel('Density')
plt.legend()
plt.show()'''

"""<h3>Lets keep it till distribution otherwise things will become time consuming and complicated. Although we can further analyze directors/cast and their similarity count of titles. But its a general project and that would be a specific task.</h3>"""

# Check for missing values in the "director" column
missing_directors = df['director'].isnull().sum()

if missing_directors > 0:
    print(f"There are {missing_directors} missing values in the 'director' column.")
else:
    print("No missing values found in the 'director' column.")

# Count the unique directors
unique_directors = df['director'].nunique()

print(f"There are {unique_directors} unique directors in your dataset.")

# Count the number of movies/shows per director
director_counts = df['director'].value_counts()

# Display the distribution (you can adjust the number of rows to display)
print(director_counts.head(10))  # Show top 10 directors

# For visualization, you can use libraries like Matplotlib or Seaborn
import matplotlib.pyplot as plt

director_counts.head(20).plot(kind='bar')  # Plot top 20 directors
plt.xlabel('Director')
plt.ylabel('Number of Movies/Shows')
plt.title('Distribution of Movies/Shows per Director')
plt.show()

# Filter for TV shows and movies
tv_shows = df[df['type'] == 'TV Show']
movies = df[df['type'] == 'Movie']

# Count the number of TV shows per director
tv_show_director_counts = tv_shows['director'].value_counts()

# Display the distribution for TV shows
print("TV Show Counts per Director:")
print(tv_show_director_counts.head(10))

# Plot the distribution for TV shows
tv_show_director_counts.head(20).plot(kind='bar')
plt.xlabel('Director')
plt.ylabel('Number of TV Shows')
plt.title('Distribution of TV Shows per Director')
plt.show()

# Count the number of movies per director
movie_director_counts = movies['director'].value_counts()

# Display the distribution for movies
print("\nMovie Counts per Director:")
print(movie_director_counts.head(10))

# Plot the distribution for movies
movie_director_counts.head(20).plot(kind='bar')
plt.xlabel('Director')
plt.ylabel('Number of Movies')
plt.title('Distribution of Movies per Director')
plt.show()

# 1. Split the 'Listed In' column and create a list of genres for each row
df['Genres_List'] = df['listed_in'].str.split(', ')

# 2. Count the occurrences of each genre for each director
genre_counts = df.groupby('director')['Genres_List'].apply(lambda x: pd.Series(x.sum()).value_counts()).unstack(fill_value=0)

# 3. Identify the dominant genre for each director
dominant_genres = genre_counts.idxmax(axis=1)
print("\nDominant Genre for each Director:\n", dominant_genres)

# 4. Calculate genre diversity for each director
genre_diversity = genre_counts.apply(lambda x: len(x[x > 0]), axis=1)
print("\nGenre Diversity for each Director:\n", genre_diversity)

# 1. Count the occurrences of each Country for each Director
country_counts = df.groupby(['director', 'country']).size().unstack(fill_value=0)

# 2. Identify the primary country for each director (where they've worked the most)
primary_countries = country_counts.idxmax(axis=1)
print("\nPrimary Country for each Director:\n", primary_countries)

# 3. Count the number of directors primarily working in each country
country_director_counts = primary_countries.value_counts()
print("\nNumber of Directors Primarily Working in each Country:\n", country_director_counts)

# 1. Count the occurrences of each Rating for each Director
rating_counts = df.groupby(['director', 'rating']).size().unstack(fill_value=0)

# 3. Identify the most frequent rating for each director
most_frequent_ratings = rating_counts.idxmax(axis=1)
print("\nMost Frequent Rating for each Director:\n", most_frequent_ratings)

import collections

# 1. Create a new DataFrame with exploded 'actor' column
df_with_actors = df.assign(individual_actor=df['cast'].str.split(', ')).explode('individual_actor')

# 2. Count the occurrences of each Actor for each Director
actor_director_counts = df_with_actors.groupby(['director', 'individual_actor']).size().reset_index(name='collaboration_count')

# 3. Identify frequent collaborators
frequent_collaborations = actor_director_counts[actor_director_counts['collaboration_count'] > 1]

# 4. Print the frequent collaborators
print("\nFrequent Collaborators:\n", frequent_collaborations)

df['duration'].unique()

print(type(df['duration']))

# 1. Preprocess the 'duration' column
def convert_duration_to_minutes(duration_str):
  if 'Season' in duration_str:
    # Assuming each season has 10 episodes and each episode is 45 minutes long
    num_seasons = int(duration_str.split(' ')[0])
    return num_seasons * 10 * 45  # Total minutes
  elif 'min' in duration_str:
    return int(duration_str.split(' ')[0])
  else:
    # Handle other potential formats or invalid values as needed
    return None  # Or some default value

df['Duration'] = df['duration'].astype(str).apply(convert_duration_to_minutes)

# 2. Drop rows with missing or invalid durations (if any)
df.dropna(subset=['Duration'], inplace=True)

# 3. Calculate the average duration for each director
average_duration = df.groupby('director')['Duration'].mean()

# 5. Print the average durations
print("\nAverage Content Duration by Director:\n", average_duration)

# Check for None values in the 'duration' column
none_count = df['duration'].isnull().sum()
print(f"Number of None values in 'duration': {none_count}")

top_directors = df['director'].value_counts().head(5).index  # Get top 5 directors

for director in top_directors:
    director_genres = df[df['director'] == director]['listed_in'].str.split(', ', expand=True).stack().value_counts()
    director_genres.plot(kind='barh')
    plt.xlabel('Count')
    plt.ylabel('Genre')
    plt.title(f'Genres Directed by {director}')
    plt.show()

# Filter for TV shows and movies
tv_shows = df[df['type'] == 'TV Show']
movies = df[df['type'] == 'Movie']

# Top directors for TV shows
top_tv_directors = tv_shows['director'].value_counts().head(5).index

for director in top_tv_directors:
    director_genres = tv_shows[tv_shows['director'] == director]['listed_in'].str.split(', ', expand=True).stack().value_counts()
    director_genres.plot(kind='barh')
    plt.xlabel('Count')
    plt.ylabel('Genre')
    plt.title(f'Genres Directed by {director} (TV Shows)')
    plt.show()

# Top directors for movies
top_movie_directors = movies['director'].value_counts().head(5).index

for director in top_movie_directors:
    director_genres = movies[movies['director'] == director]['listed_in'].str.split(', ', expand=True).stack().value_counts()
    director_genres.plot(kind='barh')
    plt.xlabel('Count')
    plt.ylabel('Genre')
    plt.title(f'Genres Directed by {director} (Movies)')
    plt.show()

for director in top_directors:
    director_ratings = df[df['director'] == director]['rating'].value_counts()
    plt.pie(director_ratings, labels=director_ratings.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Ratings Distribution for {director}')
    plt.show()

# Filter for TV shows and movies
tv_shows = df[df['type'] == 'TV Show']
movies = df[df['type'] == 'Movie']

# Top directors for TV shows
top_tv_directors = tv_shows['director'].value_counts().head(5).index

# Analyze and visualize ratings for top TV directors
for director in top_tv_directors:
    director_ratings = tv_shows[tv_shows['director'] == director]['rating'].value_counts()
    plt.pie(director_ratings, labels=director_ratings.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Ratings Distribution for {director} (TV Shows)')
    plt.show()

# Top directors for movies
top_movie_directors = movies['director'].value_counts().head(5).index

# Analyze and visualize ratings for top movie directors
for director in top_movie_directors:
    director_ratings = movies[movies['director'] == director]['rating'].value_counts()
    plt.pie(director_ratings, labels=director_ratings.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Ratings Distribution for {director} (Movies)')
    plt.show()

# 1. Create a new Series with exploded 'actor' column
exploded_actors = df['cast'].str.split(', ').explode()

# 2. Unique Cast Members
unique_cast_members = exploded_actors.nunique()
print(f"\nNumber of Unique Cast Members: {unique_cast_members}")

# 3. Top Cast Members
top_cast_members = exploded_actors.value_counts()
print("\nTop Cast Members:\n", top_cast_members)

def count_cast_members(cast_str):
    if pd.isna(cast_str):  # Handle missing values
        return 0
    else:
        return len(cast_str.split(','))

# Create a new Series with cast sizes
cast_sizes = df['cast'].apply(count_cast_members)

# Verify the correct column name for shows
# and replace 'title' if it's different
show_column = 'title'  # Adjust if needed

# Align cast_sizes with df based on their indices
cast_sizes = cast_sizes.rename('cast_size')  # Give the series a name
aligned_cast_sizes = cast_sizes.align(df[show_column], join='left')[0]  # Align and take the first Series (cast_sizes)

# Calculate average cast size per show using apply
cast_size_per_show = df.groupby(show_column)['cast'].apply(
    lambda group: aligned_cast_sizes.loc[group.index].mean()
)

print(cast_size_per_show)

def count_cast_members(cast_str):
    if pd.isna(cast_str):  # Handle missing values
        return 0
    else:
        return len(cast_str.split(','))
# 1. Cast Size vs. Type (Movie or TV Show)
df['cast_size'] = df['cast'].apply(count_cast_members)
# Calculate average cast size for each type
cast_size_by_type = df.groupby('type')['cast_size'].mean()
print("\nAverage Cast Size by Type:\n", cast_size_by_type)

# Visualize the distribution using box plots
sns.boxplot(x='type', y='cast_size', data=df)
plt.title('Cast Size Distribution by Type')
plt.show()

# 2. Cast Size vs. Rating

# Visualize the relationship using a scatter plot (adjust jitter as needed)
sns.stripplot(x='rating', y='cast_size', data=df, jitter=0.2)
plt.title('Cast Size vs. Rating')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# 3. Cast Size vs. Release Year

# Visualize the trend over time using a line plot
cast_size_by_year = df.groupby('release_year')['cast_size'].mean()
cast_size_by_year.plot(figsize=(10, 6))
plt.title('Average Cast Size Over Time')
plt.xlabel('Release Year')
plt.ylabel('Average Cast Size')
plt.show()

# 1. Cast Size vs. Type (Movie or TV Show)
# Calculate average cast size for each type
cast_size_by_type = df.groupby('type')['cast_size'].mean()
print("\nAverage Cast Size by Type:\n", cast_size_by_type)

# Visualize the distribution using box plots
sns.boxplot(x='type', y='cast_size', data=df)
plt.title('Cast Size Distribution by Type')
plt.show()

# 2. Cast Size vs. Rating (Movies and TV Shows Separately)

# Filter for movies and TV shows
movies = df[df['type'] == 'Movie']
tv_shows = df[df['type'] == 'TV Show']

# Visualize the relationship for Movies
sns.stripplot(x='rating', y='cast_size', data=movies, jitter=0.2)
plt.title('Cast Size vs. Rating (Movies)')
plt.xticks(rotation=45)
plt.show()

# Visualize the relationship for TV Shows
sns.stripplot(x='rating', y='cast_size', data=tv_shows, jitter=0.2)
plt.title('Cast Size vs. Rating (TV Shows)')
plt.xticks(rotation=45)
plt.show()

# 3. Cast Size vs. Release Year (Movies and TV Shows Separately)

# Calculate average cast size for each year for Movies
cast_size_by_year_movies = movies.groupby('release_year')['cast_size'].mean()

# Visualize the trend over time for Movies
cast_size_by_year_movies.plot(figsize=(10, 6))
plt.title('Average Cast Size Over Time (Movies)')
plt.xlabel('Release Year')
plt.ylabel('Average Cast Size')
plt.show()

# Calculate average cast size for each year for TV Shows
cast_size_by_year_tv_shows = tv_shows.groupby('release_year')['cast_size'].mean()

# Visualize the trend over time for TV Shows
cast_size_by_year_tv_shows.plot(figsize=(10, 6))
plt.title('Average Cast Size Over Time (TV Shows)')
plt.xlabel('Release Year')
plt.ylabel('Average Cast Size')
plt.show()

# 2. Count the occurrences of each unique value in the `country` column
country_counts = df['country'].value_counts().reset_index()
country_counts.columns = ['country', 'count']

# 3. Sort the countries by their frequency in descending order and select the top few countries
top_countries = country_counts.nlargest(5, 'count')  # Adjust the number as needed

# 4. Create a bar chart to visualize the distribution of the top countries
chart = alt.Chart(top_countries).mark_bar().encode(
    x=alt.X('country:N', sort='-y'),
    y=alt.Y('count:Q', title='Number of Movies/TV Shows'),
    tooltip=['country', 'count']
).properties(
    title='Top Countries by Content Count'
).interactive()

# Save the chart
chart.save('top_countries_bar_chart.json')
chart.display()

import altair as alt

# Filter data into movies and tv shows
movies = df[df['type'] == 'Movie']
tv_shows = df[df['type'] == 'TV Show']

# Count occurrences for Movies
movie_country_counts = movies['country'].value_counts().reset_index()
movie_country_counts.columns = ['country', 'count']

# Select top countries for Movies
top_movie_countries = movie_country_counts.nlargest(5, 'count')

# Create bar chart for Movies
movie_chart = alt.Chart(top_movie_countries).mark_bar().encode(
    x=alt.X('country:N', sort='-y'),
    y=alt.Y('count:Q', title='Number of Movies'),
    tooltip=['country', 'count']
).properties(
    title='Top Countries by Movie Count'
).interactive()

# Save the chart for Movies
movie_chart.save('top_movie_countries_bar_chart.json')

# Count occurrences for TV Shows
tv_show_country_counts = tv_shows['country'].value_counts().reset_index()
tv_show_country_counts.columns = ['country', 'count']

# Select top countries for TV Shows
top_tv_show_countries = tv_show_country_counts.nlargest(5, 'count')

# Create bar chart for TV Shows
tv_show_chart = alt.Chart(top_tv_show_countries).mark_bar().encode(
    x=alt.X('country:N', sort='-y'),
    y=alt.Y('count:Q', title='Number of TV Shows'),
    tooltip=['country', 'count']
).properties(
    title='Top Countries by TV Show Count'
).interactive()

# Save the chart for TV Shows
tv_show_chart.save('top_tv_show_countries_bar_chart.json')

# Display both charts
movie_chart.display()
tv_show_chart.display()

# 1. Separate the DataFrame into movies and TV shows
movies_df = df[df['type'] == 'Movie']
tv_shows_df = df[df['type'] == 'TV Show']

# 2. Count the number of movies and TV shows per country
movie_counts = movies_df['country'].value_counts()
tv_show_counts = tv_shows_df['country'].value_counts()

# 3. Plot the distributions side-by-side with adjustments for readability
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot for Movies
movie_counts.plot(kind='bar', ax=axes[0])
axes[0].set_title('Movie Counts by Country')
axes[0].set_xlabel('Country')
axes[0].set_ylabel('Count')
axes[0].tick_params(axis='x', rotation=45)

# Plot for TV Shows
tv_show_counts.plot(kind='bar', ax=axes[1])
axes[1].set_title('TV Show Counts by Country')
axes[1].set_xlabel('Country')
axes[1].set_ylabel('Count')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 4. Identify countries prevalent in specific content types
if not movie_counts.empty and not tv_show_counts.empty:
    # Reindex both Series to ensure they have the same labels
    movie_counts = movie_counts.reindex(movie_counts.index.union(tv_show_counts.index)).fillna(0)
    tv_show_counts = tv_show_counts.reindex(movie_counts.index).fillna(0)

    movie_dominant_countries = movie_counts[movie_counts > tv_show_counts].index.tolist()
    tv_show_dominant_countries = tv_show_counts[tv_show_counts > movie_counts].index.tolist()

    print("\nCountries more prevalent in producing Movies:")
    if movie_dominant_countries:
        print(*movie_dominant_countries, sep=", ")
    else:
        print("None")

    print("\nCountries more prevalent in producing TV Shows:")
    if tv_show_dominant_countries:
        filtered_tv_show_countries = [country for country in tv_show_dominant_countries if country]
        print(*filtered_tv_show_countries, sep=", ")
    else:
        print("None")
else:
    print("Not enough data to identify dominant content types by country.")

# Extract year from 'date_added' without modifying the original DataFrame
year_added_series = pd.to_datetime(df['date_added']).dt.year

# Filter data for movies and TV shows
movies_df = df[df['type'] == 'Movie']
tv_shows_df = df[df['type'] == 'TV Show']

# Group by 'country' and 'year_added', count occurrences, and unstack for movies
movie_country_year_counts = movies_df.groupby(['country', year_added_series]).size().unstack(fill_value=0)
melted_movie_df = movie_country_year_counts.reset_index().melt('country', var_name='year_added', value_name='count')
melted_movie_df['year_added'] = pd.to_numeric(melted_movie_df['year_added'])

# Group by 'country' and 'year_added', count occurrences, and unstack for TV shows
tv_show_country_year_counts = tv_shows_df.groupby(['country', year_added_series]).size().unstack(fill_value=0)
melted_tv_show_df = tv_show_country_year_counts.reset_index().melt('country', var_name='year_added', value_name='count')
melted_tv_show_df['year_added'] = pd.to_numeric(melted_tv_show_df['year_added'])

# Disable the max rows limit
alt.data_transformers.disable_max_rows()

# Create the area chart for movies
movie_chart = alt.Chart(melted_movie_df).mark_area().encode(
    x='year_added:T', # Added type here
    y=alt.Y('count:Q', title='Number of Movies'),
    color=alt.Color('country:N', legend=alt.Legend(title='Country')),
    tooltip=['country', 'year_added', 'count']
).properties(
    title='Movie Production by Country Over Time'
).interactive()

# Create the area chart for TV shows
tv_show_chart = alt.Chart(melted_tv_show_df).mark_area().encode(
    x='year_added:T', # Added type here
    y=alt.Y('count:Q', title='Number of TV Shows'),
    color=alt.Color('country:N', legend=alt.Legend(title='Country')),
    tooltip=['country', 'year_added', 'count']
).properties(
    title='TV Show Production by Country Over Time'
).interactive()

# Combine the charts vertically
combined_chart = alt.vconcat(movie_chart, tv_show_chart)

# Display the combined chart
combined_chart.save('country_vs_release_year_split_area_chart.json')
combined_chart

# Filter data for movies and TV shows
movies_df = df[df['type'] == 'Movie']
tv_shows_df = df[df['type'] == 'TV Show']

# Process data and create heatmap for movies
movie_genres_series = movies_df['listed_in'].str.split(', ')
movie_exploded_df = pd.DataFrame({'country': movies_df['country'].repeat(movie_genres_series.str.len()),
                            'genres': [genre for sublist in movie_genres_series for genre in sublist]})
movie_country_genre_counts = movie_exploded_df.groupby(['country', 'genres']).size().unstack(fill_value=0)
movie_melted_df = movie_country_genre_counts.reset_index().melt('country', var_name='genres', value_name='count')

# Aggregate the data to reduce the number of rows
movie_aggregated_df = movie_melted_df.groupby('country')['count'].sum().reset_index()

# Create the heatmap for movies
movie_chart = alt.Chart(movie_aggregated_df).mark_rect().encode(
    x='country:N',
    y='count:Q',
    color=alt.Color('count:Q', scale=alt.Scale(scheme='greens')),
    tooltip=['country', 'count']
).properties(
    title='Total Genres by Country (Movies)'
).interactive()

# Process data and create heatmap for TV shows
tv_show_genres_series = tv_shows_df['listed_in'].str.split(', ')
tv_show_exploded_df = pd.DataFrame({'country': tv_shows_df['country'].repeat(tv_show_genres_series.str.len()),
                            'genres': [genre for sublist in tv_show_genres_series for genre in sublist]})
tv_show_country_genre_counts = tv_show_exploded_df.groupby(['country', 'genres']).size().unstack(fill_value=0)
tv_show_melted_df = tv_show_country_genre_counts.reset_index().melt('country', var_name='genres', value_name='count')

# Aggregate the data to reduce the number of rows
tv_show_aggregated_df = tv_show_melted_df.groupby('country')['count'].sum().reset_index()

# Create the heatmap for TV shows
tv_show_chart = alt.Chart(tv_show_aggregated_df).mark_rect().encode(
    x='country:N',
    y='count:Q',
    color=alt.Color('count:Q', scale=alt.Scale(scheme='greens')),
    tooltip=['country', 'count']
).properties(
    title='Total Genres by Country (TV Shows)'
).interactive()

# Combine the charts vertically
combined_chart = alt.vconcat(movie_chart, tv_show_chart)

# Display the combined chart
combined_chart.save('country_vs_genre_split_heatmap.json')
combined_chart

# Get the highest rated movies and TV shows for each country
highest_rated = df.groupby(['country', 'type'])['rating'].max().reset_index()

# Filter the original dataframe to keep only the rows with the highest ratings
filtered_df = df.merge(highest_rated, on=['country', 'type', 'rating'])

# Create separate box plots for movies and TV shows
for content_type in filtered_df['type'].unique():
    # Filter data for the specific type
    type_df = filtered_df[filtered_df['type'] == content_type]

    # Create the box plot
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='country', y='rating', data=type_df, showmeans=True)

    # Set labels and title
    plt.xlabel('Country')
    plt.ylabel('Rating')
    plt.title(f'Distribution of Highest Ratings by Country ({content_type})')

    # Rotate x-axis labels for better readability if needed
    plt.xticks(rotation=45)

    # Show the plot
    plt.tight_layout()
    plt.show()

# Convert duration to minutes (assuming format '## min' or '## Seasons')
def convert_to_minutes(duration):
  if 'min' in duration:
    return int(duration.split(' ')[0])
  elif 'Seasons' in duration:
    # Estimate average season length as 10 episodes * 45 minutes/episode
    return int(duration.split(' ')[0]) * 10 * 45
  else:
    return None  # Handle unexpected formats

# Apply the conversion
df['duration_minutes'] = df['duration'].apply(convert_to_minutes)

# Calculate average duration and count of content per country and type
average_duration_by_country_and_type = df.groupby(['country', 'type'])['duration_minutes'].agg(['mean', 'count']).reset_index()

# Get top 10 countries for each type based on content count
top_10_countries_movies = average_duration_by_country_and_type[average_duration_by_country_and_type['type'] == 'Movie'].nlargest(10, 'count')['country']
top_10_countries_tv_shows = average_duration_by_country_and_type[average_duration_by_country_and_type['type'] == 'TV Show'].nlargest(10, 'count')['country']

# Create separate bar charts for movies and TV shows, filtering for top 10 countries
for content_type in average_duration_by_country_and_type['type'].unique():
    # Filter data for the specific type and top 10 countries
    type_df = average_duration_by_country_and_type[
        (average_duration_by_country_and_type['type'] == content_type) &
        (average_duration_by_country_and_type['country'].isin(top_10_countries_movies if content_type == 'Movie' else top_10_countries_tv_shows))
    ]

    # Create the bar chart
    plt.figure(figsize=(10, 6))
    sns.barplot(x='country', y='mean', data=type_df)

    # Set labels and title
    plt.xlabel('Country')
    plt.ylabel('Average Duration (minutes)')
    plt.title(f'Average {content_type} Duration by Top 10 Producing Countries')

    # Rotate x-axis labels for better readability if needed
    plt.xticks(rotation=45)

    # Show the plot
    plt.tight_layout()
    plt.show()

'''# Function to generate bar charts for top 20 names in a given column ('cast' or 'director'), separated by type
def generate_bar_chart_by_country_and_type(df, column_name, top_n=20):
    for content_type in df['type'].unique():
        # Filter data for the specific type
        type_df = df[df['type'] == content_type]

        for country in type_df['country'].unique():
            # Filter data for the specific country within the type
            country_data = type_df[type_df['country'] == country]

            # Extract and count names from the specified column
            names = [name.strip() for sublist in country_data[column_name].str.split(', ') for name in sublist if name.strip()]
            name_counts = Counter(names)

            # Get the top N most frequent names
            top_names = name_counts.most_common(top_n)

            # Prepare data for plotting
            names, counts = zip(*top_names)

            # Create the bar chart
            plt.figure(figsize=(12, 6))
            plt.bar(names, counts)

            # Set labels and title
            plt.xlabel(f'{column_name.capitalize()}')
            plt.ylabel('Count')
            plt.title(f'Top {top_n} {column_name.capitalize()} in {country} ({content_type})')

            # Rotate x-axis labels for better readability
            plt.xticks(rotation=45)

            # Show the plot
            plt.tight_layout()
            plt.show()

# Generate bar charts for top 20 'cast' and 'director', separated by 'type'
generate_bar_chart_by_country_and_type(df, 'cast')
generate_bar_chart_by_country_and_type(df, 'director')'''

# Convert 'date_added' to datetime without modifying the original DataFrame
date_added_datetime = pd.to_datetime(df['date_added'])

# Extract year and month into separate Series
year_added = date_added_datetime.dt.year
month_added = date_added_datetime.dt.month_name().tolist()  # Convert to list for compatibility

# Create a new DataFrame with the extracted information
prepared_df = pd.DataFrame({
    'original_date_added': df['date_added'],
    'year_added': year_added,
    'month_added': month_added
})

# Display the first few rows of the prepared DataFrame
print(prepared_df.head().to_string(index=False))

# Count the number of content added each year
yearly_counts = prepared_df['year_added'].value_counts().sort_index()

# Create the line chart for yearly trend
plt.figure(figsize=(10, 5))
yearly_counts.plot(kind='line', marker='o')

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Count')
plt.title('Yearly Trend of Content Added')

# Show the plot
plt.tight_layout()
plt.show()

# Convert 'date_added' to datetime format with the correct format
df['date_added'] = pd.to_datetime(df['date_added'], format='%B %d, %Y', errors = 'coerce') # Added errors = 'coerce' to handle potential errors in date format

# Extract the year from 'date_added'
df['year_added'] = df['date_added'].dt.year

# Group by 'type' and 'year' and count the entries
content_counts = df.groupby(['type', 'year_added']).size().reset_index(name='count')

# Separate Movies and TV Shows
movies_counts = content_counts[content_counts['type'] == 'Movie']
tv_shows_counts = content_counts[content_counts['type'] == 'TV Show']

# Create a line chart
plt.figure(figsize=(10, 6))
plt.plot(movies_counts['year_added'], movies_counts['count'], label='Movies', marker='o')
plt.plot(tv_shows_counts['year_added'], tv_shows_counts['count'], label='TV Shows', marker='s')
plt.xlabel('Year added')
plt.ylabel('Number of Titles')
plt.title('Netflix Content Trend: Movies vs. TV Shows')
plt.legend()
plt.grid(True)
plt.show()

'''# Count the number of content added each year
yearly_counts = prepared_df['year_added'].value_counts().sort_index()

# Create the line chart for yearly trend
plt.figure(figsize=(10, 5))
yearly_counts.plot(kind='line', marker='o')

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Count')
plt.title('Yearly Trend of Content Added')

# Show the plot
plt.tight_layout()
plt.show()
'''
# Monthly Trend

# Count the number of content added each month
monthly_counts = prepared_df['month_added'].value_counts().sort_index()

# Create the bar chart for monthly trend
plt.figure(figsize=(12, 5))
monthly_counts.plot(kind='bar')

# Set labels and title
plt.xlabel('Month Added')
plt.ylabel('Count')
plt.title('Monthly Trend of Content Added')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Filter data for movies and TV shows
movies_prepared_df = prepared_df[df['type'] == 'Movie']
tv_shows_prepared_df = prepared_df[df['type'] == 'TV Show']

'''# Yearly Trend (Movies)

# Count the number of movies added each year
yearly_movie_counts = movies_prepared_df['year_added'].value_counts().sort_index()

# Create the line chart for yearly movie trend
plt.figure(figsize=(10, 5))
yearly_movie_counts.plot(kind='line', marker='o')

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Number of Movies Added')
plt.title('Yearly Trend of Movies Added')

# Show the plot
plt.tight_layout()
plt.show()

# Yearly Trend (TV Shows)

# Count the number of TV shows added each year
yearly_tv_show_counts = tv_shows_prepared_df['year_added'].value_counts().sort_index()

# Create the line chart for yearly TV show trend
plt.figure(figsize=(10, 5))
yearly_tv_show_counts.plot(kind='line', marker='o')

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Number of TV Shows Added')
plt.title('Yearly Trend of TV Shows Added')

# Show the plot
plt.tight_layout()
plt.show()
'''
# Monthly Trend (Movies)

# Count the number of movies added each month
monthly_movie_counts = movies_prepared_df['month_added'].value_counts().sort_index()

# Create the bar chart for monthly movie trend
plt.figure(figsize=(12, 5))
monthly_movie_counts.plot(kind='bar')

# Set labels and title
plt.xlabel('Month Added')
plt.ylabel('Number of Movies Added')
plt.title('Monthly Trend of Movies Added')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Monthly Trend (TV Shows)

# Count the number of TV shows added each month
monthly_tv_show_counts = tv_shows_prepared_df['month_added'].value_counts().sort_index()

# Create the bar chart for monthly TV show trend
plt.figure(figsize=(12, 5))
monthly_tv_show_counts.plot(kind='bar')

# Set labels and title
plt.xlabel('Month Added')
plt.ylabel('Number of TV Shows Added')
plt.title('Monthly Trend of TV Shows Added')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Count the number of movies and TV shows added each year, using 'type' from the original df
yearly_type_counts = prepared_df.groupby(['year_added', df['type']]).size().unstack(fill_value=0)

# Calculate the proportion of each type for each year
yearly_type_proportions = yearly_type_counts.div(yearly_type_counts.sum(axis=1), axis=0)

# Create the stacked area chart
plt.figure(figsize=(10, 5))
yearly_type_proportions.plot(kind='area', stacked=True)

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Proportion')
plt.title('Proportion of Movies and TV Shows Added Over Time')

# Show the plot
plt.tight_layout()
plt.show()

# Filter data for movies and TV shows
movies_prepared_df = prepared_df[df['type'] == 'Movie']
tv_shows_prepared_df = prepared_df[df['type'] == 'TV Show']

# Calculate the time lag between release year and year added for movies
movies_prepared_df['time_lag'] = movies_prepared_df['year_added'] - df[df['type'] == 'Movie']['release_year']

# Calculate the time lag between release year and year added for TV shows
tv_shows_prepared_df['time_lag'] = tv_shows_prepared_df['year_added'] - df[df['type'] == 'TV Show']['release_year']

# Create separate scatter plots for movies and TV shows
for content_type, type_df in zip(['Movies', 'TV Shows'], [movies_prepared_df, tv_shows_prepared_df]):
  plt.figure(figsize=(10, 5))
  sns.scatterplot(x='year_added', y='time_lag', data=type_df)

  # Set labels and title
  plt.xlabel('Year Added')
  plt.ylabel('Time Lag (Years)')
  plt.title(f'Time Lag Between Release and Addition to Netflix ({content_type})')

  # Show the plot
  plt.tight_layout()
  plt.show()

# Count the number of content added each year for each country, using 'country' from the original df
yearly_country_counts = prepared_df.groupby(['year_added', df['country']]).size().unstack(fill_value=0)

# Get the top 5 countries based on total content count
top_5_countries = yearly_country_counts.sum().nlargest(5).index

# Filter the data to include only the top 5 countries
filtered_yearly_country_counts = yearly_country_counts[top_5_countries]

# Create the line chart for each of the top 5 countries
plt.figure(figsize=(12, 6))
for country in filtered_yearly_country_counts.columns:
    filtered_yearly_country_counts[country].plot(kind='line', marker='o', label=country)

# Set labels and title
plt.xlabel('Year Added')
plt.ylabel('Number of Content Added')
plt.title('Content Added Over Time by Top 5 Countries')

# Add legend
plt.legend()

# Show the plot
plt.tight_layout()
plt.show()

# Filter the original DataFrame into movies and TV shows
movies_df = df[df['type'] == 'Movie']
tv_shows_df = df[df['type'] == 'TV Show']

# Count for Movies
movie_yearly_counts = movies_df.groupby(['year_added', 'country']).size().unstack(fill_value=0)
top_movie_countries = movie_yearly_counts.sum().nlargest(5).index
filtered_movie_yearly_counts = movie_yearly_counts[top_movie_countries]

# Plot for Movies
plt.figure(figsize=(12, 6))
for country in filtered_movie_yearly_counts.columns:
    filtered_movie_yearly_counts[country].plot(kind='line', marker='o', label=country)
plt.xlabel('Year Added')
plt.ylabel('Number of Movies Added')
plt.title('Movie Content Added Over Time by Top 5 Countries')
plt.legend()
plt.tight_layout()
plt.show()

# Count for TV Shows
tv_show_yearly_counts = tv_shows_df.groupby(['year_added', 'country']).size().unstack(fill_value=0)
top_tv_show_countries = tv_show_yearly_counts.sum().nlargest(5).index
filtered_tv_show_yearly_counts = tv_show_yearly_counts[top_tv_show_countries]

# Plot for TV Shows
plt.figure(figsize=(12, 6))
for country in filtered_tv_show_yearly_counts.columns:
    filtered_tv_show_yearly_counts[country].plot(kind='line', marker='o', label=country)
plt.xlabel('Year Added')
plt.ylabel('Number of TV Shows Added')
plt.title('TV Show Content Added Over Time by Top 5 Countries')
plt.legend()
plt.tight_layout()
plt.show()

# Filter data for movies and TV shows
movies_df = df[df['type'] == 'Movie']
tv_shows_df = df[df['type'] == 'TV Show']

# Create histograms for movies and TV shows
for content_type, type_df in zip(['Movies', 'TV Shows'], [movies_df, tv_shows_df]):
    plt.figure(figsize=(10, 5))
    sns.histplot(type_df['release_year'], kde=True)  # kde adds a density curve for smoother visualization

    # Set labels and title
    plt.xlabel('Release Year')
    plt.ylabel('Count')
    plt.title(f'Distribution of {content_type} by Release Year')

    # Show the plot
    plt.tight_layout()
    plt.show()

# Filter for TV shows and movies
tv_shows = df[df['type'] == 'TV Show']
movies = df[df['type'] == 'Movie']

# Define a mapping for rating categories to numerical values
rating_mapping = {'TV-MA': 1, 'TV-14': 2, 'TV-PG': 3, 'R': 4, 'PG-13': 5,
                  'TV-Y': 6, 'TV-Y7': 7, 'PG': 8, 'TV-G': 9, 'G': 10,
                  'NC-17': 11, 'NR': 12, 'TV-Y7-FV': 13, 'UR': 14}

# 2. Release Year vs. Rating (TV Shows and Movies)

# TV Shows
plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='rating', data=tv_shows)
plt.title('Release Year vs. Rating (TV Shows)')
plt.xlabel('Release Year')
plt.ylabel('Rating')
plt.show()

tv_shows['rating_numeric'] = tv_shows['rating'].map(rating_mapping)
correlation = tv_shows['release_year'].corr(tv_shows['rating_numeric'])
print(f"Correlation between release year and rating (TV Shows): {correlation}")

# Movies
plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='rating', data=movies)
plt.title('Release Year vs. Rating (Movies)')
plt.xlabel('Release Year')
plt.ylabel('Rating')
plt.show()

movies['rating_numeric'] = movies['rating'].map(rating_mapping)
correlation = movies['release_year'].corr(movies['rating_numeric'])
print(f"Correlation between release year and rating (Movies): {correlation}")


# 3. Release Year vs. Duration (TV Shows and Movies)

# TV Shows - Assuming duration is in minutes for TV shows
tv_shows['duration_numeric'] = tv_shows['duration'].apply(lambda x: int(x.split()[0]) if 'min' in x else int(x.split()[0]) * 60 if 'Seasons' in x or 'Season' in x else x)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='duration_numeric', data=tv_shows)
plt.title('Release Year vs. Duration (TV Shows)')
plt.xlabel('Release Year')
plt.ylabel('Duration (Minutes)')
plt.show()

correlation = tv_shows['release_year'].corr(tv_shows['duration_numeric'])
print(f"Correlation between release year and duration (TV Shows): {correlation}")

# Movies - Define a function to convert duration to minutes
def convert_to_minutes(duration_str):
    try:
        if 'min' in duration_str:
            return int(duration_str.split()[0])
        else:
            return int(duration_str)
    except ValueError:
        return pd.NA

# Apply the conversion and fill NaN values
movies['duration_minutes'] = movies['duration'].apply(convert_to_minutes)
movies['duration_minutes'].fillna(0, inplace=True)

plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='duration_minutes', data=movies)
plt.title('Release Year vs. Duration (Movies)')
plt.xlabel('Release Year')
plt.ylabel('Duration (Minutes)')
plt.show()

correlation = movies['release_year'].corr(movies['duration_minutes'])
print(f"Correlation between release year and duration (Movies): {correlation}")

# Count the number of movies and TV shows for each release year
counts_by_year = df.groupby(['release_year', 'type']).size().unstack(fill_value=0)

# Calculate the proportion of movies for each year
counts_by_year['movie_proportion'] = counts_by_year['Movie'] / (counts_by_year['Movie'] + counts_by_year['TV Show'])

# Plot the trend
plt.figure(figsize=(10, 6))
sns.lineplot(x=counts_by_year.index, y='movie_proportion', data=counts_by_year)
plt.title('Proportion of Movies Released Over Time')
plt.xlabel('Release Year')
plt.ylabel('Proportion of Movies')
plt.show()

# Filter for movies only
movies = df[df['type'] == 'Movie']

# Count the number of movies produced by each country per year
movie_counts = movies.groupby(['release_year', 'country']).size().unstack(fill_value=0)

# Select top countries (adjust the number as needed)
top_movie_countries = movie_counts.sum().sort_values(ascending=False).head(5).index

# Filter the data for top countries
movie_counts_top = movie_counts[top_movie_countries]

# Plot the trend for top countries
plt.figure(figsize=(15, 8))
movie_counts_top.plot(kind='bar', stacked=True)
plt.title('Movie Production by Country Over Time')
plt.xlabel('Release Year')
plt.ylabel('Number of Movies')
plt.legend(title='Country')
plt.show()

# Filter for TV shows only
tv_shows = df[df['type'] == 'TV Show']

# Count the number of TV shows produced by each country per year
tv_show_counts = tv_shows.groupby(['release_year', 'country']).size().unstack(fill_value=0)

# Select top countries (adjust the number as needed)
top_tv_show_countries = tv_show_counts.sum().sort_values(ascending=False).head(5).index

# Filter the data for top countries
tv_show_counts_top = tv_show_counts[top_tv_show_countries]

# Plot the trend for top countries
plt.figure(figsize=(18, 12))
tv_show_counts_top.plot(kind='bar', stacked=True)
plt.title('TV Show Production by Country Over Time')
plt.xlabel('Release Year')
plt.ylabel('Number of TV Shows')
plt.legend(title='Country')
plt.show()

# Value Counts
value_counts = df['rating'].value_counts()

# Frequency Table (optional, but provides percentages)
frequency_table = (df['rating'].value_counts(normalize=True) * 100)

print("Value Counts:\n", value_counts)
print("\nFrequency Table:\n", frequency_table)

# Filter movies and TV shows
movies = df[df['type'] == 'Movie']
tvshows = df[df['type'] == 'TV Show']

# Value Counts for Movies
movie_value_counts = movies['rating'].value_counts()

# Frequency Table for Movies (optional)
movie_frequency_table = (movies['rating'].value_counts(normalize=True) * 100)

# Value Counts for TV Shows
tvshow_value_counts = tvshows['rating'].value_counts()

# Frequency Table for TV Shows (optional)
tvshow_frequency_table = (tvshows['rating'].value_counts(normalize=True) * 100)

print("Movie Rating Value Counts:\n", movie_value_counts)
print("\nMovie Rating Frequency Table:\n", movie_frequency_table)

print("\nTV Show Rating Value Counts:\n", tvshow_value_counts)
print("\nTV Show Rating Frequency Table:\n", tvshow_frequency_table)

# Plotting for Movies
plt.figure(figsize=(10, 5))
sns.barplot(x=movie_value_counts.index, y=movie_value_counts.values)
plt.title('Movie Ratings Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Plotting for TV Shows
plt.figure(figsize=(10, 5))

#Ensure that the cell where 'tvshow_value_counts' is defined has been run before running this cell
sns.barplot(x=tvshow_value_counts.index, y=tvshow_value_counts.values) # 'tvshow_value_counts', not 'tv_show_value_counts'

plt.title('TV Show Ratings Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# 2. Pie Charts for Movies and TV Shows

# Plotting for Movies
plt.figure(figsize=(10, 10))

# Define a threshold percentage to group less frequent categories
threshold_percentage = 2.0  # Adjust as needed

# Create a new Series for the pie chart data
movie_pie_data = movie_value_counts.copy()

# Group categories below the threshold into 'Other'
others = movie_pie_data[movie_pie_data / movie_pie_data.sum() * 100 < threshold_percentage]
movie_pie_data = movie_pie_data[movie_pie_data / movie_pie_data.sum() * 100 >= threshold_percentage]
movie_pie_data['Other'] = others.sum()

# Generate a color palette with enough colors for all slices (including 'Other')
colors = sns.color_palette('husl', n_colors=len(movie_pie_data))

# Explode slices for better visibility (adjust as needed)
explode = [0.05 if i < 5 else 0 for i in range(len(movie_pie_data))]

# Use smaller font size and adjust label distance
textprops = {'fontsize': 8}
pctdistance = 0.9

# Plot the pie chart with label rotation and adjusted startangle
wedges, _, labels = plt.pie(movie_pie_data.values, labels=movie_pie_data.index, autopct='%1.1f%%',
                            startangle=160, colors=colors, explode=explode, textprops=textprops,
                            pctdistance=pctdistance, rotatelabels=True)

plt.title('Movie Ratings Proportion')
plt.show()

# Plotting for TV Shows
plt.figure(figsize=(10, 10))

# Define a threshold percentage to group less frequent categories
threshold_percentage = 2.0  # Adjust as needed

# Create a new Series for the pie chart data
tvshow_pie_data = tvshow_value_counts.copy() # Changed 'tv_show_pie_data' to 'tvshow_pie_data'

# Group categories below the threshold into 'Other'
others = tvshow_pie_data[tvshow_pie_data / tvshow_pie_data.sum() * 100 < threshold_percentage] # Changed 'tv_show_pie_data' to 'tvshow_pie_data'
tvshow_pie_data = tvshow_pie_data[tvshow_pie_data / tvshow_pie_data.sum() * 100 >= threshold_percentage] # Changed 'tv_show_pie_data' to 'tvshow_pie_data'
tvshow_pie_data['Other'] = others.sum() # Changed 'tv_show_pie_data' to 'tvshow_pie_data'

# Generate a color palette with enough colors for all slices (including 'Other')
colors = sns.color_palette('husl', n_colors=len(tvshow_pie_data)) # Changed 'tv_show_pie_data' to 'tvshow_pie_data'

# Explode slices for better visibility (adjust as needed)
explode = [0.05 if i < 5 else 0 for i in range(len(tvshow_pie_data))] # Changed 'tv_show_pie_data' to 'tvshow_pie_data'

# Use smaller font size and adjust label distance
textprops = {'fontsize': 8}
pctdistance = 0.9

# Plot the pie chart with label rotation and adjusted startangle
wedges, _, labels = plt.pie(tvshow_pie_data.values, labels=tvshow_pie_data.index, autopct='%1.1f%%',  # Changed 'tv_show_pie_data' to 'tvshow_pie_data'
                            startangle=160, colors=colors, explode=explode, textprops=textprops,
                            pctdistance=pctdistance, rotatelabels=True)

plt.title('TV Show Ratings Proportion')
plt.show()

# 1. Create a cross tabulation of `rating` and `type` columns and display it
cross_tab = pd.crosstab(df['rating'], df['type'])
print("Cross tabulation of rating and type:\n", cross_tab)

# 2. Create a grouped bar chart with `rating` on x-axis, count on y-axis and `type` as hue
chart = alt.Chart(df).mark_bar().encode(
    x=alt.X('rating:N', axis=alt.Axis(title='Rating')),
    y=alt.Y('count()', axis=alt.Axis(title='Count')),
    color='type:N',
    tooltip=['rating', 'type', 'count()']
)

# 3. Add appropriate labels and title
chart = chart.properties(
    title='Distribution of Ratings by Type'
).interactive()

# 4. Display the plot (save as JSON)
chart.save('rating_by_type_bar_chart.json')

# 5. Perform Chi-Square test of independence on `rating` and `type` and display the results
chi2, p, dof, expected = chi2_contingency(cross_tab)
print("\nChi-Square Test Results:")
print("Chi2 Statistic:", chi2)
print("P-value:", p)
print("Degrees of Freedom:", dof)

# Make a copy of the 'listed_in' column to avoid modifying the original
df['genres'] = df['listed_in'].str.split(', ')

# Explode the dataframe using the new column
exploded_df = df.explode('genres')

# 1. Grouped Bar Chart: Count of each rating within each genre
chart1 = alt.Chart(exploded_df).mark_bar().encode(
    x=alt.X('genres:N', axis=alt.Axis(title='Genre')),
    y=alt.Y('count()', axis=alt.Axis(title='Count')),
    color='rating:N',
    tooltip=['genres', 'rating', 'count()']
).properties(
    title='Distribution of Ratings across Genres'
).interactive()

# 3. Top Ratings per Genre (using the new column)
top_ratings_per_genre = exploded_df.groupby('genres')['rating'].agg(lambda x: x.value_counts().index[0]).reset_index()
print("\nTop Ratings per Genre:\n", top_ratings_per_genre)

# Display the plots
chart1.save('rating_by_genre_bar_chart.json')
chart1.display()

# Make a copy of the 'listed_in' column to avoid modifying the original
df['genres'] = df['listed_in'].str.split(', ')

# Explode the dataframe using the new column
exploded_df = df.explode('genres')

# 1. Grouped Bar Chart: Count of each rating within each genre
chart1 = alt.Chart(exploded_df).mark_bar().encode(
    x=alt.X('genres:N', axis=alt.Axis(title='Genre')),
    y=alt.Y('count()', axis=alt.Axis(title='Count')),
    color='rating:N',
    tooltip=['genres', 'rating', 'count()']
).properties(
    title='Distribution of Ratings across Genres'
).interactive()

# 2. Top Ratings per Genre (using the new column)
top_ratings_per_genre = exploded_df.groupby('genres')['rating'].agg(lambda x: x.value_counts().index[0]).reset_index()
print("\nTop Ratings per Genre:\n", top_ratings_per_genre)

# Display the plot
chart1.save('rating_by_genre_bar_chart.json')

chart1.display()

# 1. Data Preprocessing: Extract numerical duration and convert to numeric

# Extract numerical part and unit from `duration`
df[['duration_num', 'duration_unit']] = df['duration'].str.split(' ', expand=True)

# Convert `duration_num` to numeric (handle non-numeric values)
df['duration_num'] = pd.to_numeric(df['duration_num'], errors='coerce')

# 2. Convert Seasons to Minutes (Assuming 1 Season = 10 hours = 600 minutes)
df.loc[df['duration_unit'] == 'Seasons', 'duration_min'] = df['duration_num'] * 600
df.loc[df['duration_unit'] == 'min', 'duration_min'] = df['duration_num']

# 3. Descriptive Statistics by Type

# Calculate summary statistics for Movies and TV Shows separately
movie_duration_stats = df[df['type'] == 'Movie']['duration_min'].describe()
tv_show_duration_stats = df[df['type'] == 'TV Show']['duration_min'].describe()

print("\nDescriptive Statistics for Movie Durations:\n", movie_duration_stats)
print("\nDescriptive Statistics for TV Show Durations:\n", tv_show_duration_stats)

# 4. Visualizations

# Side-by-side Box Plots
sns.boxplot(x='type', y='duration_min', data=df, showmeans=True)
plt.title('Duration Distribution by Type (Box Plots)')
plt.xlabel('Type')
plt.ylabel('Duration (Minutes)')
plt.show()

# Histograms (Separate)
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
sns.histplot(df[df['type'] == 'Movie']['duration_min'], kde=True)
plt.title('Movie Duration Distribution')
plt.xlabel('Duration (Minutes)')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
sns.histplot(df[df['type'] == 'TV Show']['duration_min'], kde=True)
plt.title('TV Show Duration Distribution')
plt.xlabel('Duration (Minutes)')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# Density Plots (Overlay)
sns.kdeplot(df[df['type'] == 'Movie']['duration_min'], label='Movie', shade=True)
sns.kdeplot(df[df['type'] == 'TV Show']['duration_min'], label='TV Show', shade=True)
plt.title('Duration Density by Type')
plt.xlabel('Duration (Minutes)')
plt.ylabel('Density')
plt.legend()
plt.show()

# 1. Data Preprocessing: Handle multiple genres in 'listed_in'

# Explode 'listed_in' to have one genre per row
df_exploded = df.explode('listed_in')

# 2. Descriptive Statistics by Genre

# Calculate summary statistics for each genre
genre_duration_stats = df_exploded.groupby('listed_in')['duration_min'].describe()

# Print the summary statistics
print("\nDescriptive Statistics by Genre:\n", genre_duration_stats)

# 3. Visualizations

# Grouped Box Plots
plt.figure(figsize=(12, 6))
sns.boxplot(x='listed_in', y='duration_min', data=df_exploded, showmeans=True)
plt.title('Duration Distribution by Genre (Box Plots)')
plt.xlabel('Genre')
plt.ylabel('Duration (Minutes)')
plt.xticks(rotation=45)
plt.show()

# Bar Chart of Average Durations
plt.figure(figsize=(12, 6))
sns.barplot(x='listed_in', y='duration_min', data=df_exploded, ci=None)  # ci=None removes error bars
plt.title('Average Duration by Genre')
plt.xlabel('Genre')
plt.ylabel('Average Duration (Minutes)')
plt.xticks(rotation=45)
plt.show()

# 1. Descriptive Statistics by Rating

# Calculate summary statistics for each rating category
rating_duration_stats = df.groupby('rating')['duration_min'].describe()

# Print the summary statistics
print("\nDescriptive Statistics by Rating:\n", rating_duration_stats)

# 2. Visualizations

# Grouped Box Plots
plt.figure(figsize=(12, 6))
sns.boxplot(x='rating', y='duration_min', data=df, showmeans=True)
plt.title('Duration Distribution by Rating (Box Plots)')
plt.xlabel('Rating')
plt.ylabel('Duration (Minutes)')
plt.xticks(rotation=45)
plt.show()

# 3. Scatter Plot (if `rating` is ordinal)

# Define an ordinal mapping for rating categories (adjust as needed)
rating_order = ['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA', 'NR', 'UR']

# Convert `rating` to ordered categorical
df['rating'] = pd.Categorical(df['rating'], categories=rating_order, ordered=True)

# Create scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='rating', y='duration_min', data=df)
plt.title('Duration vs. Rating (Scatter Plot)')
plt.xlabel('Rating')
plt.ylabel('Duration (Minutes)')
plt.xticks(rotation=45)
plt.show()

# 1. Calculate Average/Median Duration by Release Year

# Calculate average duration per release year
avg_duration_by_year = df.groupby('release_year')['duration_min'].mean().reset_index()

# Calculate median duration per release year (optional)
median_duration_by_year = df.groupby('release_year')['duration_min'].median().reset_index()

# 2. Visualizations

# Line Chart (Average Duration)
plt.figure(figsize=(10, 6))
sns.lineplot(x='release_year', y='duration_min', data=avg_duration_by_year)
plt.title('Average Duration Trend Over the Years')
plt.xlabel('Release Year')
plt.ylabel('Average Duration (Minutes)')
plt.show()

# Scatter Plot (Average Duration) - Optional
plt.figure(figsize=(10, 6))
sns.scatterplot(x='release_year', y='duration_min', data=avg_duration_by_year)
plt.title('Average Duration vs. Release Year (Scatter Plot)')
plt.xlabel('Release Year')
plt.ylabel('Average Duration (Minutes)')
plt.show()

# Calculate correlation between 'release_year' and 'duration_min'
# Ensure both columns have the same length after handling NaN values

# Drop rows with NaN values in 'duration_min' for both columns
df_cleaned = df.dropna(subset=['duration_min'])

correlation, p_value = pearsonr(df_cleaned['release_year'], df_cleaned['duration_min'])

print("\nCorrelation between Release Year and Duration:", correlation)
print("P-value:", p_value)

# 1. Check Data Type
print(df['listed_in'].dtype)

# 2. Inspect Unique Values (First few for overview)
print("\nSample Unique Values in 'listed_in':")
print(df['listed_in'].unique()[:5])  # Display first 5 unique values

# 3. Check for Null Values
print("\nNumber of Null Values:", df['listed_in'].isnull().sum())

# 4. Frequency Counts (Top Genres)

# Split comma-separated genres and count occurrences
all_genres = ','.join(df['listed_in']).split(',')
genre_counts = pd.Series(all_genres).value_counts()

print("\nTop Genres:\n", genre_counts.head(10))  # Display top 10 genres

# 1. Handle Multiple Genres (Explode into Separate Rows)

# Explode 'listed_in' to have one genre per row
df_exploded = df.assign(listed_in=df['listed_in'].str.split(', ')).explode('listed_in')

# 2. Standardize Genre Names (Example: Replace "Sci-Fi" with "Science Fiction")

# Create a dictionary for genre name replacements (add more as needed)
genre_replacements = {'Sci-Fi': 'Science Fiction'}

# Replace genre names using the dictionary
df_exploded['listed_in'] = df_exploded['listed_in'].replace(genre_replacements)

# 3. Handle Missing Values (Impute with "Unknown")

# Fill null values with "Unknown"
df_exploded['listed_in'] = df_exploded['listed_in'].fillna('Unknown')

# Display the first few rows of the cleaned DataFrame
print(df_exploded.head().to_string(index=False))

# 1. Bar Chart of Genre Frequencies

# Count genre occurrences
genre_counts = df_exploded['listed_in'].value_counts()

# Plot bar chart
plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts.index, y=genre_counts.values)
plt.title('Genre Frequency Bar Chart')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# 2. Word Cloud of Genres

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(genre_counts)

# Plot word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# 3. Genre Co-occurrence

# Prepare data for co-occurrence analysis
genre_lists = df['listed_in'].str.split(', ').tolist()

# Encode transactions
te = TransactionEncoder()
te_ary = te.fit(genre_lists).transform(genre_lists)
df_genres = pd.DataFrame(te_ary, columns=te.columns_)

# Apply Apriori algorithm
frequent_itemsets = apriori(df_genres, min_support=0.02, use_colnames=True)  # Adjust min_support as needed

# Print frequent itemsets (co-occurring genres)
print("\nFrequent Itemsets (Co-occurring Genres):\n")
print(frequent_itemsets)

# --- For TV Shows ---

# Filter data for TV Shows
df_tv_shows = df_exploded[df_exploded['type'] == 'TV Show']

# 1. Bar Chart of Genre Frequencies (TV Shows)

# Count genre occurrences for TV Shows
genre_counts_tv = df_tv_shows['listed_in'].value_counts()

# Plot bar chart for TV Shows
plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts_tv.index, y=genre_counts_tv.values)
plt.title('TV Show Genre Frequency Bar Chart')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.xticks(rotation=90)  # Adjust rotation as needed
plt.show()

# 2. Word Cloud of Genres (TV Shows)

# Generate word cloud for TV Shows
wordcloud_tv = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(genre_counts_tv)

# Plot word cloud for TV Shows
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_tv, interpolation='bilinear')
plt.axis('off')
plt.show()

# 3. Genre Co-occurrence (TV Shows)

# Prepare data for co-occurrence analysis (TV Shows)
tv_show_genre_lists = df[df['type'] == 'TV Show']['listed_in'].str.split(', ').tolist()

# Encode transactions (TV Shows)
te_tv = TransactionEncoder()
te_ary_tv = te_tv.fit(tv_show_genre_lists).transform(tv_show_genre_lists)
df_genres_tv = pd.DataFrame(te_ary_tv, columns=te_tv.columns_)

# Apply Apriori algorithm (TV Shows)
frequent_itemsets_tv = apriori(df_genres_tv, min_support=0.02, use_colnames=True)

# Print frequent itemsets for TV Shows
print("\nFrequent Itemsets (Co-occurring Genres) for TV Shows:\n")
print(frequent_itemsets_tv)

# --- For Movies ---

# Filter data for Movies
df_movies = df_exploded[df_exploded['type'] == 'Movie']

# 1. Bar Chart of Genre Frequencies (Movies)

# Count genre occurrences for Movies
genre_counts_movies = df_movies['listed_in'].value_counts()

# Plot bar chart for Movies
plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts_movies.index, y=genre_counts_movies.values)
plt.title('Movie Genre Frequency Bar Chart')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.xticks(rotation=90)  # Adjust rotation as needed
plt.show()

# 2. Word Cloud of Genres (Movies)

# Generate word cloud for Movies
wordcloud_movies = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(genre_counts_movies)

# Plot word cloud for Movies
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_movies, interpolation='bilinear')
plt.axis('off')
plt.show()

# 3. Genre Co-occurrence (Movies)

# Prepare data for co-occurrence analysis (Movies)
movie_genre_lists = df[df['type'] == 'Movie']['listed_in'].str.split(', ').tolist()

# Encode transactions (Movies)
te_movies = TransactionEncoder()
te_ary_movies = te_movies.fit(movie_genre_lists).transform(movie_genre_lists)
df_genres_movies = pd.DataFrame(te_ary_movies, columns=te_movies.columns_)

# Apply Apriori algorithm (Movies)
frequent_itemsets_movies = apriori(df_genres_movies, min_support=0.02, use_colnames=True)

# Print frequent itemsets for Movies
print("\nFrequent Itemsets (Co-occurring Genres) for Movies:\n")
print(frequent_itemsets_movies)

# Count the number of unique genres per year
genre_diversity = df.groupby('release_year')['genres'].apply(lambda x: x.explode().unique().shape[0]).reset_index(name='num_unique_genres')

# Visualize the trend
plt.figure(figsize=(10, 6))
plt.plot(genre_diversity['release_year'], genre_diversity['num_unique_genres'], marker='o')
plt.title('Genre Diversity Over Time')
plt.xlabel('Release Year')
plt.ylabel('Number of Unique Genres')
plt.grid(True)
plt.show()

# Preprocessing: Extract individual genres from 'listed_in'
df['genres'] = df['listed_in'].str.split(', ')
exploded_df = df.explode('genres')

# Group by release year and genre, count occurrences
genre_trends = exploded_df.groupby(['release_year', 'genres']).size().reset_index(name='count')

# Pivot the data for easier visualization
genre_trends_pivot = genre_trends.pivot(index='release_year', columns='genres', values='count').fillna(0)

# Select top N genres to visualize (adjust as needed)
top_genres = genre_trends_pivot.sum().sort_values(ascending=False).head(5).index
genre_trends_top = genre_trends_pivot[top_genres]

# Visualize trends
plt.figure(figsize=(12, 8))
genre_trends_top.plot(figsize=(12, 8))
plt.title('Genre Trends Over Release Years')
plt.xlabel('Release Year')
plt.ylabel('Count')
plt.legend(title='Genres')
plt.grid(True)
plt.show()

# Define the columns to keep
columns_to_keep = ['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description']

# Drop the columns not in the 'columns_to_keep' list
columns_to_drop = [col for col in df.columns if col not in columns_to_keep]
df.drop(columns_to_drop, axis=1, inplace=True)

df.dropna()

def check_for_nan_null_inf(df, columns):
  """
  Checks for NaN, null (None), and infinite values in specified columns of a DataFrame.

  Args:
    df: The DataFrame to check.
    columns: A list of column names to check.

  Returns:
    A new DataFrame where each row represents a column, and the columns indicate
    whether that column contains NaN, null, or infinite values.
  """
  import numpy as np # import numpy
  results = {}
  for col in columns:
    results[col] = {
        'has_nan': df[col].isna().any(),
        'has_null': df[col].isnull().any(),
        'has_inf': (
            pd.to_numeric(df[col], errors='coerce')
            .isin([np.inf, -np.inf]) # use np.inf instead of pd.np.inf
            .any()
        ),
    }

  return pd.DataFrame(results).transpose()

check_for_nan_null_inf(df, columns=['description', 'title', 'listed_in', 'country', 'rating', 'release_year', 'type'])

"""# **DATA STORY TELLING AFTER EDA**

<h1>The Netflix Story Unfolded</h1>

<h2>1. A Universe of Content:
Netflix boasts a vast library, with movies reigning supreme, accounting for 75.6% of the total content. TV shows, while fewer in number, hold their own, comprising 24.4%.</h2>
<h2>2. Maturity Evolves:
Over time, we see a clear shift towards more mature content, particularly in TV shows. Recent years have seen a surge in TV-MA rated shows, catering to audiences seeking edgier and more complex narratives.</h2>
<h2>3. Duration Dynamics:
Movie durations vary greatly, reflecting the diverse nature of cinematic storytelling. We see longer durations in genres like dramas and documentaries, while shorter durations are prevalent in comedies and children's films.
TV shows have seen a subtle increase in duration, particularly for TV-MA rated series, perhaps indicating a growing trend towards more immersive and serialized narratives.</h2>
<h2>4. Global Content Landscape:
The United States remains a dominant force in content production, contributing the majority of both movies and TV shows on Netflix.
However, the global stage is evolving. We observe a significant rise in content from other countries, notably India, which has witnessed a substantial increase in movie production, adding diversity to Netflix's library.</h2>
<h2>5. Cast Size Chronicles:
Movies tend to boast larger casts compared to TV shows, aligning with their grander scale and production values.
Interestingly, larger casts are also often associated with more mature ratings, particularly in movies, suggesting a correlation between ensemble storytelling and complex themes.</h2>
<h2>6. The Textual Tapestry:
Most titles on Netflix are concise, capturing attention with brevity. Movie titles, on average, are slightly longer than TV show titles.
Word frequency analysis reveals common themes and subjects, with words like 'love', 'man', and 'world' appearing frequently in titles across both movies and TV shows.</h2>
<h2>7. Sentiment in Titles:
Titles generally strike a positive tone, hinting at the optimistic and entertaining nature of the content.
This positive sentiment remains consistent across various genres and ratings, suggesting a broad appeal to audiences seeking uplifting narratives.</h2>
<h2>8. Behind the Scenes:
A few prolific directors, such as Ral Campos & Jan Suter and Marcus Raboy, have made their mark on both movies and TV shows on Netflix.
The majority of directors contribute a smaller number of titles, indicating a diverse and dynamic creative landscape.</h2>
<h2>9. The Evolution of Acquisition:
Netflix has become increasingly agile in acquiring content, with the time lag between a movie's release and its addition to the platform shrinking over time.
This faster acquisition process allows audiences to enjoy newer movies and TV shows sooner, enhancing the platform's appeal.</h2>

# **PRE-PROCESSING AND FEATURE ENGINEERING**
"""

# Group by 'description' and count occurrences
duplicate_descriptions = df.groupby('description').size().reset_index(name='count')

# Filter rows where count > 1 to identify duplicates
duplicates = duplicate_descriptions[duplicate_descriptions['count'] > 1]

# Display rows with duplicate descriptions along with the count
print(duplicates)

# Find descriptions that are duplicated
duplicate_descriptions = df.groupby('description').size().reset_index(name='count')
duplicates = duplicate_descriptions[duplicate_descriptions['count'] > 1]

# Filter the original DataFrame to include only rows with duplicate descriptions
duplicate_rows = df[df['description'].isin(duplicates['description'])]

# Display all rows with the same descriptions
print(duplicate_rows)

# Drop duplicates based on the 'description' column, keeping the first occurrence
df= df.drop_duplicates(subset='description', keep='first')

# Step 1: Find the index of the row containing ', France, Algeria'
row_index = df[df['country'] == ', France, Algeria'].index[0]

# Step 2: Remove the leading comma
df.at[row_index, 'country'] = df.at[row_index, 'country'].lstrip(',')

# Verify the result
print(df.loc[row_index])

# Define replacements
replacements = {
    'United States of America': 'United States'
}

# Replace values in the 'country' column
for old, new in replacements.items():
    df['country'] = df['country'].replace(old, new, regex=True)

# Verify the result
print(df['country'].unique())

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def clean_text(text):
    """
    Cleans the text by lowercasing, removing punctuation, stop words, and lemmatizing.

    Args:
        text: The text string to be cleaned.

    Returns:
        The cleaned text string.
    """

    # Lowercasing
    text = text.lower()

    # Punctuation removal
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenization
    words = nltk.word_tokenize(text)

    # Stop word removal
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join the words back into a string
    cleaned_text = ' '.join(words)

    return cleaned_text

# Assuming 'df' is your DataFrame
df['description'] = df['description'].astype(str).apply(clean_text)

'''from sklearn.preprocessing import OrdinalEncoder
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse

# 1. One-hot encode 'type'
df_encoded = pd.get_dummies(df, columns=['type'])

# 2. Ordinal encode 'rating'
ordinal_encoder = OrdinalEncoder()
df['rating_encoded'] = ordinal_encoder.fit_transform(df[['rating']])

# 3. Multi-label binarize 'country'
mlb_country = MultiLabelBinarizer()
country_encoded = mlb_country.fit_transform(df['country'].str.split(', '))
country_df = pd.DataFrame(country_encoded, columns=mlb_country.classes_, index=df_encoded.index)
df_encoded = df_encoded.join(country_df)

# 4. Multi-label binarize 'listed_in'
mlb_listed_in = MultiLabelBinarizer()
listed_in_encoded = mlb_listed_in.fit_transform(df['listed_in'].str.split(', '))
listed_in_df = pd.DataFrame(listed_in_encoded, columns=mlb_listed_in.classes_, index=df_encoded.index)
df_encoded = df_encoded.join(listed_in_df)

# 5. Drop the original 'country', 'listed_in', and 'rating' columns (but keep 'title' and 'show_id')
df_encoded.drop(['country', 'listed_in', 'rating'], axis=1, inplace=True)

# 6. Vectorize the 'description' column using TF-IDF
tfidf_vectorizer = TfidfVectorizer()
description_matrix = tfidf_vectorizer.fit_transform(df['description'])

# 7. Combine the encoded DataFrame with the 'rating_encoded' column and drop 'description' (but keep 'title' and 'show_id')
df_encoded.drop('description', axis=1, inplace=True)

# 8. Convert the 'rating_encoded' column to a NumPy array
rating_encoded = df[['rating_encoded']].values

# 9. Combine features into a single sparse matrix
# Convert all values to numeric type before creating the sparse matrix
combined_matrix = sparse.hstack([
    sparse.csr_matrix(df_encoded.drop(['title', 'show_id'], axis=1).values.astype(float)),  # Encoded categorical features except 'title' and 'show_id'
    description_matrix,  # TF-IDF matrix for 'description'
    sparse.csr_matrix(rating_encoded)  # Ordinal encoded 'rating'
])

# 10. Print the shape of the combined_matrix to verify the size
print("Combined matrix shape:", combined_matrix.shape)

# 11. Optionally print the first 5 rows of the combined matrix
print(combined_matrix[:5])'''
from sklearn.preprocessing import OrdinalEncoder
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse

# 1. One-hot encode 'type'
df_encoded = pd.get_dummies(df, columns=['type'])

# 2. Ordinal encode 'rating'
ordinal_encoder = OrdinalEncoder()
df['rating_encoded'] = ordinal_encoder.fit_transform(df[['rating']])

# 3. Multi-label binarize 'country'
mlb_country = MultiLabelBinarizer()
country_encoded = mlb_country.fit_transform(df['country'].str.split(', '))
country_df = pd.DataFrame(country_encoded, columns=mlb_country.classes_, index=df_encoded.index)
df_encoded = df_encoded.join(country_df)

# 4. Multi-label binarize 'listed_in'
mlb_listed_in = MultiLabelBinarizer()
listed_in_encoded = mlb_listed_in.fit_transform(df['listed_in'].str.split(', '))
listed_in_df = pd.DataFrame(listed_in_encoded, columns=mlb_listed_in.classes_, index=df_encoded.index)
df_encoded = df_encoded.join(listed_in_df)

# 5. Drop the original 'country', 'listed_in', and 'rating' columns (but keep 'title' and 'show_id')
df_encoded.drop(['country', 'listed_in', 'rating'], axis=1, inplace=True)

# 6. Vectorize the 'description' column using TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
description_matrix = tfidf_vectorizer.fit_transform(df['description'])

# 7. Vectorize the 'director' column using TF-IDF
director_tfidf = tfidf_vectorizer.fit_transform(df['director'].apply(lambda x: ', '.join(x.split(', '))))

# 8. Drop the original 'description' and 'director' columns from df_encoded
df_encoded.drop(['description', 'director'], axis=1, inplace=True)


# 9. Convert the 'rating_encoded' column to a NumPy array
rating_encoded = df[['rating_encoded']].values

# 10. Combine features into a single sparse matrix
# Convert all values to numeric type before creating the sparse matrix
combined_matrix = sparse.hstack([
    sparse.csr_matrix(df_encoded.drop(['title', 'show_id','cast','date_added','release_year','duration'], axis=1).values.astype(float)),  # Encoded categorical features except 'title' and 'show_id'
    description_matrix,  # TF-IDF matrix for 'description'
    director_tfidf,      # TF-IDF matrix for 'director'
    sparse.csr_matrix(rating_encoded)  # Ordinal encoded 'rating'
])

# 11. Print the shape of the combined_matrix to verify the size
print("Combined matrix shape:", combined_matrix.shape)

# 12. Optionally print the first 5 rows of the combined matrix
print(combined_matrix[:5])

# Convert the combined_matrix (sparse) to a dense NumPy array
X = combined_matrix.toarray()

# Optionally, print the shape of X to verify
print("X shape:", X.shape)

"""# **PRINCIPAL COMPONENT ANALYSIS(FOR DIMENSIONALITY REDUCTION)**"""

# Using PCA to reduce dimensionality
pca = PCA(random_state=40)
pca.fit(X)

# Explained variance for different number of components
plt.figure(figsize=(10,5))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.title('PCA - Cumulative explained variance vs number of components')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')

# reducing the dimensions to 4000 using pca
pca = PCA(n_components=1500,random_state=40)
pca.fit(X)

# transformed features
x_pca = pca.transform(X)

# Print explained variance
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total explained variance:", sum(pca.explained_variance_ratio_))

# Print the shape of the PCA-transformed feature matrix
print("PCA-transformed combined matrix shape:", x_pca.shape)

"""# **SILHOUETTE SCORE AND ELBOW METHOD**"""

range_n_clusters = range(2, 25)
silhouette_avg = []

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(x_pca) + (n_clusters + 1) * 10])

    # Initialize the clusterer with the updated parameters
    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', random_state=40)
    cluster_labels = clusterer.fit_predict(x_pca)

    # Silhouette score for this n_clusters
    silhouette_avg_value = silhouette_score(x_pca, cluster_labels)
    silhouette_avg.append(silhouette_avg_value)
    print(f"For n_clusters = {n_clusters}, The average silhouette_score is : {silhouette_avg_value}")

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(x_pca, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")
    ax1.axvline(x=silhouette_avg_value, color="red", linestyle="--")
    ax1.set_yticks([])
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(x_pca[:, 0], x_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker=f'${i}$', alpha=1, s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
    plt.suptitle(f"Silhouette analysis for KMeans clustering on sample data with n_clusters = {n_clusters}", fontsize=14, fontweight='bold')

plt.show()

# Plot silhouette score for different number of clusters
plt.figure(figsize=(10, 5))
plt.plot(range_n_clusters, silhouette_avg)
plt.xlabel('Values of K')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k - KMeans clustering')
plt.show()

# Elbow method for finding optimal number of clusters
sum_of_sq_dist = {}
for k in range(2, 25):
    km = KMeans(n_clusters=k, init='k-means++', max_iter=1000)
    km = km.fit(X)
    sum_of_sq_dist[k] = km.inertia_

# Plot the graph for the sum of square distance values and number of clusters
plt.figure(figsize=(10, 5))
sns.pointplot(x=list(sum_of_sq_dist.keys()), y=list(sum_of_sq_dist.values()))
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""# **MODELLING WITH 2 CLUSTERS(K MEANS)**"""

# Clustering the data into 2 clusters
kmeans = KMeans(n_clusters=2,init='k-means++',random_state=40)
kmeans.fit(x_pca)

# Adding a kmeans cluster number attribute
df['kmeans_cluster=2'] = kmeans.labels_

# Evaluation metrics - distortion, Silhouette score
kmeans_distortion = kmeans.inertia_
kmeans_silhouette_score = silhouette_score(x_pca, kmeans.labels_)

print((kmeans_distortion,kmeans_silhouette_score))

# Number of movies and tv shows in each cluster
plt.figure(figsize=(10,5))
q = sns.countplot(x='kmeans_cluster=2',data=df, hue='type')
plt.title('Number of movies and TV shows in each cluster - Kmeans Clustering')
for i in q.patches:
  q.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

import seaborn as sns
import matplotlib.pyplot as plt

# Count the number of items in each cluster
plt.figure(figsize=(10, 5))
ax = sns.countplot(x='kmeans_cluster=2', data=df)
plt.title('Number of items in each cluster - Kmeans Clustering')

# Annotate the bars with the counts
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                xytext = (0, 10),
                textcoords = 'offset points')

plt.show()

"""# **MODELLING WITH 4 CLUSTERS(K MEANS)**"""

# Clustering the data into 4 clusters
kmeans = KMeans(n_clusters=4,init='k-means++',random_state=40)
kmeans.fit(x_pca)

# Adding a kmeans cluster number attribute
df['kmeans_cluster=4'] = kmeans.labels_

# Evaluation metrics - distortion, Silhouette score
kmeans_distortion = kmeans.inertia_
kmeans_silhouette_score = silhouette_score(x_pca, kmeans.labels_)

print((kmeans_distortion,kmeans_silhouette_score))

# Number of movies and tv shows in each cluster
plt.figure(figsize=(10,5))
q = sns.countplot(x='kmeans_cluster=4',data=df, hue='type')
plt.title('Number of movies and TV shows in each cluster - Kmeans Clustering')
for i in q.patches:
  q.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

# Count the number of items in each cluster
plt.figure(figsize=(10, 5))
ax = sns.countplot(x='kmeans_cluster=4', data=df)
plt.title('Number of items in each cluster - Kmeans Clustering')

# Annotate the bars with the counts
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                xytext = (0, 10),
                textcoords = 'offset points')

plt.show()

"""# **DENDROGRAM VISUALIZATION**"""

# Generate the linkage matrix
linkage_matrix = shc.linkage(x_pca, method='ward')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
plt.title("Dendrogram")
dend = shc.dendrogram(linkage_matrix)

# Customize axes labels
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distance")
plt.show()

"""# **HIERARCHICAL CLUSTERING WITH 2 CLUSTERS**"""

from sklearn.cluster import AgglomerativeClustering
# Fitting hierarchical clustering model
hierarchical = AgglomerativeClustering(n_clusters=2, metric='euclidean', linkage='ward')
hierarchical.fit_predict(x_pca)

# Adding a kmeans cluster number attribute
df['hierarchical_cluster=2'] = hierarchical.labels_

# Number of movies and tv shows in each cluster
plt.figure(figsize=(10,5))
q = sns.countplot(x='hierarchical_cluster=2',data=df, hue='type')
plt.title('Number of movies and tv shows in each cluster - Hierarchical Clustering')
for i in q.patches:
  q.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

# Count the number of items in each cluster
plt.figure(figsize=(10, 5))
ax = sns.countplot(x='hierarchical_cluster=2', data=df)  # Remove hue='type'
plt.title('Number of items in each cluster - Hierarchical Clustering')

# Annotate the bars with the counts
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center',
                xytext = (0, 10),
                textcoords = 'offset points')

plt.show()

"""# **BUILDING CONTENT BASED RECOMMENDER SYSTEM FOR THE AUDIENCE USING PCA AS THE METHOD OF REDUCING FEATURES**"""

# Step 1: TF-IDF vectorization for 'listed_in', 'description', and 'director'
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
listed_in_tfidf = tfidf_vectorizer.fit_transform(df['listed_in'])
description_tfidf = tfidf_vectorizer.fit_transform(df['description'])
director_tfidf = tfidf_vectorizer.fit_transform(df['director'])

# Step 2: Combine numeric features
numeric_features = df[['rating_encoded', 'kmeans_cluster=2', 'kmeans_cluster=4', 'hierarchical_cluster=2']].values

# Step 3: Concatenate all features (TF-IDF and numeric)
feature_matrix = hstack([listed_in_tfidf, description_tfidf, director_tfidf, numeric_features])

# Print the shape of the feature matrix
print("Feature matrix shape:", feature_matrix.shape)

# Convert the feature_matrix (sparse) to a dense NumPy array
X = feature_matrix.toarray()

# Optionally, print the shape of X to verify
print("X shape:", X.shape)

# Step 1: Fit PCA without specifying n_components, so we can plot explained variance for all components
pca = PCA(random_state=40)
pca.fit(X)

# Step 2: Plot the explained variance for different numbers of components
plt.figure(figsize=(10, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.title('PCA - Cumulative explained variance vs number of components')
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

# Show the plot to help decide n_components
plt.grid(True)
plt.show()

threshold = 0.80
# Find the number of components that meet the threshold
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
n_components = np.argmax(cumulative_variance_ratio >= threshold) + 1

print(n_components)

# reducing the dimensions to 202 using pca
pca = PCA(n_components=202,random_state=40)
pca.fit(X)

# transformed features
x_pca = pca.transform(X)

# Print explained variance
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total explained variance:", sum(pca.explained_variance_ratio_))

# Print the shape of the PCA-transformed feature matrix
print("PCA-transformed feature matrix shape:", x_pca.shape)

# Step 4: Calculate cosine similarity using PCA-transformed features
cosine_sim = cosine_similarity(x_pca)

# Step 5: Recommender function based on title
def get_recommendations(title, cosine_sim=cosine_sim, df=df):
    # Get the index of the show that matches the title
    idx = df.index[df['title'] == title].tolist()[0]

    # Get similarity scores for all shows with that show
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort shows based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the indices of the top 5 most similar shows (excluding itself)
    sim_scores = sim_scores[1:6]

    # Get the show_ids and titles of the most similar shows
    show_indices = [i[0] for i in sim_scores]
    recommended_shows = df[['show_id', 'title']].iloc[show_indices]

    return recommended_shows

# Example usage with error handling
try:
    recommended_shows = get_recommendations('3%')
    print(recommended_shows)
except IndexError:
    print("The title '3%' was not found in the dataset.")